{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_csv_predict3.0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtrInKWnIQc7",
        "outputId": "1a4c61bb-61e8-454d-dd1f-65121987414b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nkONmdcTNRPW",
        "outputId": "609cc91a-b2fb-4df5-d2ae-62fc43857431"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q \"https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\" > /dev/null\n",
        "!tar -xvf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install spark-nlp and pyspark\n",
        "! pip install spark-nlp==3.0.0 pyspark==3.1.1\n",
        "\n",
        "# Quick SparkSession start\n",
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version\")\n",
        "sparknlp.version()\n",
        "print(\"Apache Spark version\")\n",
        "spark.version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spark-3.1.1-bin-hadoop2.7/\n",
            "spark-3.1.1-bin-hadoop2.7/NOTICE\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/tests/\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/tests/python_executable_check.py\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/tests/autoscale.py\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/tests/worker_memory_check.py\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/tests/py_container_checks.py\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/tests/decommissioning.py\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/tests/pyfiles.py\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/tests/decommissioning_cleanup.py\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/spark/\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/spark/decom.sh\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/spark/entrypoint.sh\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n",
            "spark-3.1.1-bin-hadoop2.7/kubernetes/dockerfiles/spark/Dockerfile\n",
            "spark-3.1.1-bin-hadoop2.7/jars/\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/RoaringBitmap-0.9.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-digester-1.8.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hive-vector-code-gen-2.3.7.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/derby-10.12.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-beanutils-1.9.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/okhttp-3.12.12.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/httpcore-4.4.12.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/logging-interceptor-3.12.12.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/scala-library-2.12.10.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-mllib-local_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/parquet-format-2.4.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kryo-shaded-4.0.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/xercesImpl-2.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-logging-1.1.3.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-compiler-3.0.16.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jdo-api-3.0.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spire-macros_2.12-0.17.0-M1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/json4s-core_2.12-3.7.0-M5.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/JLargeArrays-1.5.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jsp-api-2.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-autoscaling-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.30.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/json4s-ast_2.12-3.7.0-M5.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hive-cli-2.3.7.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/parquet-common-1.10.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/stax-api-1.0.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jackson-dataformat-yaml-2.10.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hive-storage-api-2.7.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/algebra_2.12-2.0.0-M2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hadoop-annotations-2.7.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/guice-servlet-3.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spire-util_2.12-0.17.0-M1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jakarta.activation-api-1.2.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jline-2.14.6.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/breeze_2.12-1.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/metrics-jvm-4.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/machinist_2.12-0.6.8.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/scala-compiler-2.12.10.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-pool-1.5.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-network-shuffle_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-repl_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jta-1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-admissionregistration-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/istack-commons-runtime-3.0.8.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/metrics-graphite-4.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hadoop-hdfs-2.7.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-streaming_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-launcher_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/objenesis-2.6.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-collections-3.2.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/audience-annotations-0.5.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jpam-1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jakarta.annotation-api-1.3.5.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/xmlenc-0.52.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-common-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-extensions-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/py4j-0.10.9.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spire-platform_2.12-0.17.0-M1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/avro-1.8.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/curator-client-2.7.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/javax.inject-1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-apiextensions-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jersey-media-jaxb-2.30.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/javax.jdo-3.2.0-m3.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hadoop-auth-2.7.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-io-2.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/json4s-jackson_2.12-3.7.0-M5.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/paranamer-2.8.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/cats-kernel_2.12-2.0.0-M4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-mllib_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/flatbuffers-java-1.9.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jersey-server-2.30.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/stream-2.9.6.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/datanucleus-api-jdo-4.2.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/gson-2.2.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-hive-thriftserver_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/xml-apis-1.4.01.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/antlr4-runtime-4.8-1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-policy-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hive-jdbc-2.3.7.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hive-exec-2.3.7-core.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/opencsv-2.3.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-certificates-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/avro-mapred-1.8.2-hadoop2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jsr305-3.0.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/macro-compat_2.12-1.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-sketch_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-dbcp-1.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jersey-container-servlet-core-2.30.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/okio-1.14.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hk2-api-2.6.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/parquet-jackson-1.10.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/mesos-1.4.0-shaded-protobuf.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/orc-mapreduce-1.5.12.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/chill-java-0.9.5.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-settings-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-tags_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jakarta.servlet-api-4.0.3.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/automaton-1.11-8.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-lang-2.6.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jackson-annotations-2.10.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hk2-utils-2.6.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/velocity-1.5.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jul-to-slf4j-1.7.30.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/JTransforms-3.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jersey-client-2.30.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-catalyst_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/httpclient-4.5.6.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-discovery-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jetty-sslengine-6.1.26.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spire_2.12-0.17.0-M1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jersey-hk2-2.30.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-sql_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-math3-3.4.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/xbean-asm7-shaded-4.15.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-network-common_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jakarta.validation-api-2.0.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/parquet-encoding-1.10.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/snappy-java-1.1.8.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/slf4j-log4j12-1.7.30.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/slf4j-api-1.7.30.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hive-shims-0.23-2.3.7.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/guava-14.0.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/stax-api-1.0-2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-kubernetes_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/zjsonpatch-0.3.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hive-service-rpc-3.1.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/shapeless_2.12-2.3.3.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-graphx_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/oro-2.0.8.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/arrow-memory-netty-2.0.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-scheduling-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/libfb303-0.9.3.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/core-1.1.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jersey-container-servlet-2.30.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/datanucleus-rdbms-4.1.19.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/super-csv-2.2.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jackson-module-paranamer-2.10.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/aopalliance-1.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/osgi-resource-locator-1.0.3.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jackson-module-scala_2.12-2.10.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hive-shims-2.3.7.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/json-1.8.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/antlr-runtime-3.5.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/threeten-extra-1.5.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/orc-shims-1.5.12.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jetty-6.1.26.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/arrow-vector-2.0.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jackson-module-jaxb-annotations-2.10.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jersey-common-2.30.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/aircompressor-0.10.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/lz4-java-1.7.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hadoop-client-2.7.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/activation-1.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-crypto-1.1.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-tags_2.12-3.1.1-tests.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/libthrift-0.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/HikariCP-2.5.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/generex-1.0.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/breeze-macros_2.12-1.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jaxb-runtime-2.3.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/json4s-scalap_2.12-3.7.0-M5.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-events-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-lang3-3.10.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jackson-datatype-jsr310-2.11.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-httpclient-3.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-client-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-hive_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/shims-0.9.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/scala-xml_2.12-1.2.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hk2-locator-2.6.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hadoop-common-2.7.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/orc-core-1.5.12.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/curator-recipes-2.7.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hive-common-2.3.7.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/xz-1.5.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/ST4-4.0.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hive-shims-scheduler-2.3.7.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-yarn_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-batch-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/netty-all-4.1.51.Final.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/parquet-hadoop-1.10.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/chill_2.12-0.9.5.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-configuration-1.6.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jackson-databind-2.10.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/scala-reflect-2.12.10.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/joda-time-2.10.5.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/minlog-1.3.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jakarta.ws.rs-api-2.1.6.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/aopalliance-repackaged-2.6.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-compress-1.20.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-core_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jodd-core-3.5.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-storageclass-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jackson-core-2.10.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-core-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-net-3.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/datanucleus-core-4.1.17.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/janino-3.0.16.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hive-shims-common-2.3.7.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/javolution-5.5.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-rbac-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/transaction-api-1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-metrics-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-cli-1.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jakarta.inject-2.6.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/metrics-jmx-4.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/zstd-jni-1.4.8-1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jakarta.xml.bind-api-2.3.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-codec-1.10.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/curator-framework-2.7.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hive-metastore-2.3.7.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hive-beeline-2.3.7.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/metrics-core-4.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hive-llap-common-2.3.7.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/arrow-format-2.0.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-kvstore_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/javassist-3.25.0-GA.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/scala-collection-compat_2.12-2.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/arrow-memory-core-2.0.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/parquet-column-1.10.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/jetty-util-6.1.26.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-coordination-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/avro-ipc-1.8.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/guice-3.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/commons-text-1.6.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-apps-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/metrics-json-4.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/snakeyaml-1.24.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/ivy-2.4.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.4.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/univocity-parsers-2.9.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/pyrolite-4.30.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/spark-mesos_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/log4j-1.2.17.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/hive-serde-2.3.7.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/scala-parser-combinators_2.12-1.1.2.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/kubernetes-model-networking-4.12.0.jar\n",
            "spark-3.1.1-bin-hadoop2.7/jars/zookeeper-3.4.14.jar\n",
            "spark-3.1.1-bin-hadoop2.7/data/\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/sample_lda_data.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/sample_svm_data.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/sample_fpgrowth.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/iris_libsvm.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/sample_movielens_data.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/als/\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/als/test.data\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/pic_data.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/kittens/\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/kittens/54893.jpg\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/kittens/not-image.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/kittens/DP802813.jpg\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/kittens/DP153539.jpg\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/license.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/multi-channel/\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA.png\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/multi-channel/grayscale.jpg\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA.png\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA_alpha_60.png\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/grayscale.jpg\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/chr30.4.184.jpg\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/not-image.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/54893.jpg\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP802813.jpg\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP153539.jpg\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/images/license.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/ridge-data/\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/ridge-data/lpsa.data\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/kmeans_data.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/pagerank_data.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/mllib/gmm_data.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/graphx/\n",
            "spark-3.1.1-bin-hadoop2.7/data/graphx/users.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/graphx/followers.txt\n",
            "spark-3.1.1-bin-hadoop2.7/data/streaming/\n",
            "spark-3.1.1-bin-hadoop2.7/data/streaming/AFINN-111.txt\n",
            "spark-3.1.1-bin-hadoop2.7/R/\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/sparkr.zip\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/tests/\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/tests/testthat/\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/DESCRIPTION\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/profile/\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/profile/shell.R\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/profile/general.R\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/INDEX\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/Meta/\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/Meta/features.rds\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/Meta/links.rds\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/Meta/package.rds\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/help/\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/help/aliases.rds\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/help/AnIndex\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/help/paths.rds\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/R/\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/R/SparkR\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/NAMESPACE\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/html/\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/html/00Index.html\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/html/R.css\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/worker/\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/worker/worker.R\n",
            "spark-3.1.1-bin-hadoop2.7/R/lib/SparkR/worker/daemon.R\n",
            "spark-3.1.1-bin-hadoop2.7/README.md\n",
            "spark-3.1.1-bin-hadoop2.7/RELEASE\n",
            "spark-3.1.1-bin-hadoop2.7/yarn/\n",
            "spark-3.1.1-bin-hadoop2.7/yarn/spark-3.1.1-yarn-shuffle.jar\n",
            "spark-3.1.1-bin-hadoop2.7/LICENSE\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/start-workers.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/start-master.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/workers.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/start-worker.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/spark-config.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/start-history-server.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/start-slaves.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/spark-daemon.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/stop-worker.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/stop-mesos-dispatcher.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/decommission-worker.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/start-mesos-shuffle-service.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/decommission-slave.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/slaves.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/stop-history-server.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/start-thriftserver.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/stop-thriftserver.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/start-slave.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/start-all.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/stop-slave.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/spark-daemons.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/stop-workers.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/stop-slaves.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/stop-all.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/start-mesos-dispatcher.sh\n",
            "spark-3.1.1-bin-hadoop2.7/sbin/stop-master.sh\n",
            "spark-3.1.1-bin-hadoop2.7/examples/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/survreg.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/glm.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/prefixSpan.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/lm_with_elastic_net.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/powerIterationClustering.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/lda.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/kstest.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/isoreg.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/ml.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/fmRegressor.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/mlp.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/als.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/kmeans.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/svmLinear.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/fmClassifier.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/logit.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/randomForest.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/gbt.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/decisionTree.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/ml/fpm.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/dataframe.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/data-manipulation.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/streaming/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/AccumulatorMetricsTest.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VarianceThresholdSelectorExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnivariateFeatureSelectorExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FMRegressorExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FMClassifierExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RobustScalerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKerberizedKafkaWordCount.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKerberizedKafkaWordCount.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedScalar.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/people.json\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/users.avro\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/people.csv\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/users.parquet\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/users.orc\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/dir1/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/dir1/file1.parquet\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/dir1/dir2/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/dir1/dir2/file2.parquet\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/dir1/file3.json\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/user.avsc\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/full_user.avsc\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/kv1.txt\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/people.txt\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/resources/employees.json\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFMRegressorExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaUnivariateFeatureSelectorExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVarianceThresholdSelectorExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKerberizedKafkaWordCount.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedScalar.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKerberizedKafkaWordCount.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scripts/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/scripts/getGpusResources.sh\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/kmeans.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/dct_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/cross_validator.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/fm_regressor_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/univariate_feature_selector_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/linearsvc.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/lda_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/vector_size_hint_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/robust_scaler_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/power_iteration_clustering_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/pca_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/rformula_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/als_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/onehot_encoder_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/summarizer_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/feature_hasher_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/imputer_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/fm_classifier_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/variance_threshold_selector_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/correlation_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/prefixspan_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/ml/interaction_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/logistic_regression.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/als.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/word2vec.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/svd_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/kmeans.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/correlations.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/wordcount.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/status_api_demo.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/pagerank.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sort.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/transitive_closure.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/avro_inputformat.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/pi.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/streaming/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sql/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sql/datasource.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sql/hive.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sql/arrow.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sql/streaming/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/sql/basic.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/src/main/python/parquet_inputformat.py\n",
            "spark-3.1.1-bin-hadoop2.7/examples/jars/\n",
            "spark-3.1.1-bin-hadoop2.7/examples/jars/spark-examples_2.12-3.1.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/examples/jars/scopt_2.12-3.7.1.jar\n",
            "spark-3.1.1-bin-hadoop2.7/conf/\n",
            "spark-3.1.1-bin-hadoop2.7/conf/metrics.properties.template\n",
            "spark-3.1.1-bin-hadoop2.7/conf/workers.template\n",
            "spark-3.1.1-bin-hadoop2.7/conf/fairscheduler.xml.template\n",
            "spark-3.1.1-bin-hadoop2.7/conf/log4j.properties.template\n",
            "spark-3.1.1-bin-hadoop2.7/conf/spark-defaults.conf.template\n",
            "spark-3.1.1-bin-hadoop2.7/conf/spark-env.sh.template\n",
            "spark-3.1.1-bin-hadoop2.7/bin/\n",
            "spark-3.1.1-bin-hadoop2.7/bin/sparkR.cmd\n",
            "spark-3.1.1-bin-hadoop2.7/bin/sparkR\n",
            "spark-3.1.1-bin-hadoop2.7/bin/spark-submit\n",
            "spark-3.1.1-bin-hadoop2.7/bin/pyspark2.cmd\n",
            "spark-3.1.1-bin-hadoop2.7/bin/spark-class\n",
            "spark-3.1.1-bin-hadoop2.7/bin/pyspark.cmd\n",
            "spark-3.1.1-bin-hadoop2.7/bin/spark-submit2.cmd\n",
            "spark-3.1.1-bin-hadoop2.7/bin/load-spark-env.cmd\n",
            "spark-3.1.1-bin-hadoop2.7/bin/spark-sql\n",
            "spark-3.1.1-bin-hadoop2.7/bin/docker-image-tool.sh\n",
            "spark-3.1.1-bin-hadoop2.7/bin/find-spark-home.cmd\n",
            "spark-3.1.1-bin-hadoop2.7/bin/load-spark-env.sh\n",
            "spark-3.1.1-bin-hadoop2.7/bin/pyspark\n",
            "spark-3.1.1-bin-hadoop2.7/bin/spark-shell.cmd\n",
            "spark-3.1.1-bin-hadoop2.7/bin/spark-shell2.cmd\n",
            "spark-3.1.1-bin-hadoop2.7/bin/spark-submit.cmd\n",
            "spark-3.1.1-bin-hadoop2.7/bin/beeline.cmd\n",
            "spark-3.1.1-bin-hadoop2.7/bin/find-spark-home\n",
            "spark-3.1.1-bin-hadoop2.7/bin/spark-class.cmd\n",
            "spark-3.1.1-bin-hadoop2.7/bin/sparkR2.cmd\n",
            "spark-3.1.1-bin-hadoop2.7/bin/beeline\n",
            "spark-3.1.1-bin-hadoop2.7/bin/spark-class2.cmd\n",
            "spark-3.1.1-bin-hadoop2.7/bin/spark-sql.cmd\n",
            "spark-3.1.1-bin-hadoop2.7/bin/run-example\n",
            "spark-3.1.1-bin-hadoop2.7/bin/spark-shell\n",
            "spark-3.1.1-bin-hadoop2.7/bin/run-example.cmd\n",
            "spark-3.1.1-bin-hadoop2.7/bin/spark-sql2.cmd\n",
            "spark-3.1.1-bin-hadoop2.7/python/\n",
            "spark-3.1.1-bin-hadoop2.7/python/.gitignore\n",
            "spark-3.1.1-bin-hadoop2.7/python/run-tests-with-coverage\n",
            "spark-3.1.1-bin-hadoop2.7/python/mypy.ini\n",
            "spark-3.1.1-bin-hadoop2.7/python/pylintrc\n",
            "spark-3.1.1-bin-hadoop2.7/python/MANIFEST.in\n",
            "spark-3.1.1-bin-hadoop2.7/python/README.md\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_coverage/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_coverage/coverage_daemon.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_coverage/conf/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_coverage/conf/spark-defaults.conf\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_coverage/sitecustomize.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/run-tests.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/setup.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/userlibrary.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/hello/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/hello/sub_hello/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/hello/hello.txt\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/userlib-0.1.zip\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/SimpleHTTPServer.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/people.json\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/people_array.json\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/people_array_utf16le.json\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/text-test.txt\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/ages.csv\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_metadata\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_common_metadata\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/ages_newlines.csv\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/streaming/\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/streaming/text-test.txt\n",
            "spark-3.1.1-bin-hadoop2.7/python/test_support/sql/people1.json\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_rddbarrier.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_worker.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_serializers.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_util.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_rdd.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_broadcast.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_appsubmit.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_profiler.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_pin_thread.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_install_spark.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_shuffle.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_join.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_taskcontext.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_context.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_readwrite.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_conf.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/tests/test_daemon.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/__init__.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/_typing.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/testing/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/testing/mlutils.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/testing/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/testing/mllibutils.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/testing/utils.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/testing/sqlutils.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/testing/streamingutils.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/accumulators.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/rddsampler.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/install.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/status.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_algorithms.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_evaluation.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_util.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_feature.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_pipeline.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_wrapper.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_tuning.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_persistence.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_param.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_training_summary.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_linalg.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_image.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_stat.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tests/test_base.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/evaluation.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/_typing.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/functions.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/recommendation.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/fpm.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/feature.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/stat.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/stat.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/image.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/classification.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/common.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/pipeline.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/recommendation.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/clustering.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/regression.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/param/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/param/__init__.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/param/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/param/shared.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/param/shared.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/feature.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/classification.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tree.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/util.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tuning.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/fpm.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/regression.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/functions.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/base.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/image.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/tree.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/clustering.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/common.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/linalg/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/evaluation.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/ml/util.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/context.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/find_spark_home.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/serializers.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/java_gateway.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/traceback_utils.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/tests/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/tests/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/tests/test_resources.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/profile.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/information.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/requests.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/information.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/requests.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/resource/profile.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/conf.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/resultiterable.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/version.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/files.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tests/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tests/test_algorithms.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tests/test_streaming_algorithms.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tests/test_util.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tests/test_feature.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tests/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tests/test_linalg.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tests/test_stat.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/evaluation.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/_typing.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/recommendation.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/fpm.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/feature.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/classification.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/common.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/random.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/recommendation.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/clustering.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/regression.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/feature.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/classification.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tree.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/util.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/fpm.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/regression.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/random.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/test.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/test.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/tree.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/clustering.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/common.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/linalg/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/evaluation.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/mllib/util.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/resultiterable.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/profiler.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/statcounter.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/join.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/daemon.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/rdd.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/context.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/cloudpickle/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/cloudpickle/cloudpickle_fast.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/cloudpickle/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/cloudpickle/compat.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/cloudpickle/cloudpickle.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/storagelevel.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/version.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/py.typed\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/files.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/worker.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/statcounter.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/conf.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/shell.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/tests/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/tests/test_listener.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/tests/test_kinesis.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/tests/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/tests/test_dstream.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/tests/test_context.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/dstream.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/kinesis.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/context.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/dstream.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/listener.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/kinesis.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/listener.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/context.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/streaming/util.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/accumulators.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/profiler.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/status.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/broadcast.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_functions.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_readwriter.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_utils.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_grouped_map.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_dataframe.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_map.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_udf.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_streaming.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_serde.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_window.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_group.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_cogrouped_map.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_grouped_agg.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_scalar.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_catalog.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_datasources.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_pandas_udf_typehints.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_types.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_column.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_context.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_conf.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_arrow.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/tests/test_session.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/__init__.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/map_ops.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/functions.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/serializers.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/typehints.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/group_ops.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/map_ops.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/types.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/group_ops.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/functions.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/utils.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/_typing/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/_typing/__init__.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/_typing/protocols/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/_typing/protocols/__init__.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/_typing/protocols/series.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/_typing/protocols/frame.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/_typing.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/functions.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/streaming.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/context.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/column.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/catalog.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/types.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/window.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/udf.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/conf.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/session.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/column.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/group.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/catalog.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/group.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/context.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/dataframe.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/types.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/functions.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/conf.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/udf.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/avro/\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/avro/functions.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/avro/__init__.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/avro/functions.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/readwriter.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/window.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/session.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/streaming.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/shuffle.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/rdd.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/taskcontext.pyi\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/taskcontext.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/_globals.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/broadcast.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/util.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/pyspark/storagelevel.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/.coveragerc\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/make2.bat\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/reference/\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/reference/pyspark.ss.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/reference/index.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/reference/pyspark.ml.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/reference/pyspark.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/reference/pyspark.mllib.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/reference/pyspark.streaming.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/reference/pyspark.sql.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/reference/pyspark.resource.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/index.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/getting_started/\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/getting_started/index.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/getting_started/quickstart.ipynb\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/getting_started/install.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/conf.py\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/_templates/\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/_templates/autosummary/\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/_templates/autosummary/class.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/_templates/autosummary/class_with_docs.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/_static/\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/_static/copybutton.js\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/_static/css/\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/_static/css/pyspark.css\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/development/\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/development/setting_ide.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/development/index.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/development/debugging.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/development/testing.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/development/contributing.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/user_guide/\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/user_guide/arrow_pandas.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/user_guide/index.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/user_guide/python_packaging.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/migration_guide/\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_2.3.0_to_2.3.1_above.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/migration_guide/index.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_2.4_to_3.0.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_1.0_1.2_to_1.3.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_2.3_to_2.4.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_1.4_to_1.5.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/source/migration_guide/pyspark_2.2_to_2.3.rst\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/make.bat\n",
            "spark-3.1.1-bin-hadoop2.7/python/docs/Makefile\n",
            "spark-3.1.1-bin-hadoop2.7/python/lib/\n",
            "spark-3.1.1-bin-hadoop2.7/python/lib/PY4J_LICENSE.txt\n",
            "spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip\n",
            "spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip\n",
            "spark-3.1.1-bin-hadoop2.7/python/run-tests\n",
            "spark-3.1.1-bin-hadoop2.7/python/setup.cfg\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-respond.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-antlr.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-dagre-d3.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-pyrolite.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-sorttable.js.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-janino.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-protobuf.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-jquery.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-scopt.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-netlib.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-datatables.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-pmml-model.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-paranamer.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-jakarta-ws-rs-api\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-dnsjava.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-jakarta.xml.bind-api.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-jakarta-annotation-api\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-CC0.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-jodd.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-f2j.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-machinist.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-javolution.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-modernizr.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-spire.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-leveldbjni.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-join.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-zstd-jni.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-slf4j.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-arpack.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-jsp-api.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-JTransforms.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-JLargeArrays.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-bootstrap.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-reflectasm.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-javassist.html\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-zstd.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-json-formatter.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-matchMedia-polyfill.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-scala.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-jakarta.activation-api.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-automaton.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-javax-transaction-transaction-api.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-jaxb-runtime.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-minlog.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-mustache.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-xmlenc.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-jline.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-istack-commons-runtime.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-py4j.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-vis-timeline.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-re2j.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-kryo.txt\n",
            "spark-3.1.1-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt\n",
            "openjdk version \"1.8.0_282\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_282-8u282-b08-0ubuntu1~18.04-b08)\n",
            "OpenJDK 64-Bit Server VM (build 25.282-b08, mixed mode)\n",
            "Collecting spark-nlp==3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/ae/3241dc181d667b8c32be6ba441901b848b14484e3888af618ab6e039967a/spark_nlp-3.0.0-py2.py3-none-any.whl (140kB)\n",
            "\u001b[K     || 143kB 5.8MB/s \n",
            "\u001b[?25hCollecting pyspark==3.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/b0/9d6860891ab14a39d4bddf80ba26ce51c2f9dc4805e5c6978ac0472c120a/pyspark-3.1.1.tar.gz (212.3MB)\n",
            "\u001b[K     || 212.3MB 20kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     || 204kB 48.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=71778cbd52cdaeff3e89975117bd3956bb2bab36e8c319423aa2a18dae7ab9b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/90/c0/01de724414ef122bd05f056541fb6a0ecf47c7ca655f8b3c0f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: spark-nlp, py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.1 spark-nlp-3.0.0\n",
            "Spark NLP version\n",
            "Apache Spark version\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.1.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJun2Z7RNRhL"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "import sparknlp\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.base import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bk8xeSHNRuo"
      },
      "source": [
        "def start(gpu=False):\n",
        "    builder = SparkSession.builder \\\n",
        "        .appName(\"Spark NLP\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.driver.memory\", \"8G\") \\\n",
        "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
        "        .config(\"spark.kryoserializer.buffer.max\", \"1000M\")\n",
        "    if gpu:\n",
        "        builder.config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.5.1\")\n",
        "    else:\n",
        "        builder.config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.1\")\n",
        "\n",
        "    return builder.getOrCreate()\n",
        "  \n",
        "spark = start(gpu=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBtj6IPe6BLH",
        "outputId": "261e121a-7edc-4798-cf5d-c284831b376d"
      },
      "source": [
        "bert = BertEmbeddings.pretrained('bert_base_cased', 'en')\\\n",
        ".setInputCols([\"sentence\",'token'])\\\n",
        ".setOutputCol(\"bert\")\\\n",
        ".setCaseSensitive(True)\\\n",
        "# .tokenizer.encode(text, add_special_tokens=True, max_length=600)\n",
        "# .setPoolingLayer(0) # default 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_base_cased download started this may take some time.\n",
            "Approximate size to download 389.1 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV38NPAs6RNn"
      },
      "source": [
        "nerTagger = NerDLApproach()\\\n",
        ".setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
        ".setLabelColumn(\"label\")\\\n",
        ".setOutputCol(\"ner\")\\\n",
        ".setMaxEpochs(1)\\\n",
        ".setRandomSeed(0)\\\n",
        ".setVerbose(1)\\\n",
        ".setValidationSplit(0.2)\\\n",
        ".setEvaluationLogExtended(True)\\\n",
        ".setEnableOutputLogs(True)\\\n",
        ".setIncludeConfidence(True)\\\n",
        "# .setTestDataset(\"test_withEmbeds.parquet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe8BKGOgXZDV"
      },
      "source": [
        "loaded_ner_model = NerDLModel.load(\"/content/drive/MyDrive/BERT_model\") \\\n",
        "   .setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
        "   .setOutputCol(\"ner\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEAO1H-oXacj",
        "outputId": "d32d77b0-52f6-48b9-c3d0-7224c754074d"
      },
      "source": [
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentence = SentenceDetector()\\\n",
        "    .setInputCols(['document'])\\\n",
        "    .setOutputCol('sentence')\n",
        "\n",
        "token = Tokenizer()\\\n",
        "    .setInputCols(['sentence'])\\\n",
        "    .setOutputCol('token')\n",
        "\n",
        "bert = BertEmbeddings.pretrained('bert_base_cased', 'en') \\\n",
        " .setInputCols([\"sentence\",'token'])\\\n",
        " .setOutputCol(\"bert\")\\\n",
        " .setCaseSensitive(True)\n",
        "\n",
        "converter = NerConverter()\\\n",
        "  .setInputCols([\"sentence\",\"document\", \"token\"])\\\n",
        "  .setOutputCol(\"ner_span\")\n",
        "\n",
        "ner_prediction_pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        sentence,\n",
        "        token,\n",
        "        bert,\n",
        "        loaded_ner_model,\n",
        "        converter])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_base_cased download started this may take some time.\n",
            "Approximate size to download 389.1 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3fohQmyoriS"
      },
      "source": [
        "def makeTuple(ner_tags,combined,tupleAdd, paragraph): # code duplication : works for now, try to make it better\n",
        "  \"\"\"\n",
        "  args: ner_tags - list, combined - list of starts and ends of the predicted sentence(paragraph),\n",
        "        tupleAdd - final dictionary to append to json - {'tagged_entities': [[195, 206, 'Section  302', 'Section']} - format: start, end, actual token, type of token.\n",
        "        paragraph - string - the current paragraph including para,subPara, and blockQuotes - used for slicing to find actual tokens\n",
        "  returns: updated tupleAdd - dict\n",
        "  \"\"\"\n",
        "  count_article = False\n",
        "  count_section = False\n",
        "  count_case = False\n",
        "  count_act = False\n",
        "  count_cite = False\n",
        "\n",
        "  local_article = list()\n",
        "  local_section = list()\n",
        "  local_case = list()\n",
        "  local_act = list()\n",
        "  local_cite = list()\n",
        "\n",
        "  for i in range(0,len(ner_tags)):\n",
        "    if ner_tags[i].find(\"O\") != -1:\n",
        "      if count_section == True:\n",
        "        tupleAdd['tagged_entities'].append([local_section[0],local_section[-1],str(paragraph[local_section[0]:local_section[-1]+1]),\"Section\"])\n",
        "        local_section = list()\n",
        "        count_section = False\n",
        "      if count_case == True:\n",
        "        tupleAdd['tagged_entities'].append([local_case[0],local_case[-1],str(paragraph[local_case[0]:local_case[-1]+1]),\"Case\"])\n",
        "        local_case = list()\n",
        "        count_case = False\n",
        "      if count_act == True:\n",
        "        tupleAdd['tagged_entities'].append([local_act[0],local_act[-1],str(paragraph[local_act[0]:local_act[-1]+1]),\"Act\"])\n",
        "        local_act = list()\n",
        "        count_act = False\n",
        "      if count_article == True:\n",
        "        tupleAdd['tagged_entities'].append([local_article[0],local_article[-1],str(paragraph[local_article[0]:local_article[-1]+1]),\"Article\"])\n",
        "        local_article = list()\n",
        "        count_article = False\n",
        "      if count_cite == True:\n",
        "        tupleAdd['tagged_entities'].append([local_cite[0],local_cite[-1],str(paragraph[local_cite[0]:local_cite[-1]+1]),\"Cite\"])\n",
        "        local_cite = list()\n",
        "        count_cite = False\n",
        "\n",
        "    # article check\n",
        "    elif ner_tags[i].find(\"Article\") != -1:\n",
        "      if count_section == True:\n",
        "        tupleAdd['tagged_entities'].append([local_section[0],local_section[-1],str(paragraph[local_section[0]:local_section[-1]+1]),\"Section\"])\n",
        "        local_section = list()\n",
        "      if count_case == True:\n",
        "        tupleAdd['tagged_entities'].append([local_case[0],local_case[-1],str(paragraph[local_case[0]:local_case[-1]+1]),\"Case\"])\n",
        "        local_case = list()\n",
        "      if count_act == True:\n",
        "        tupleAdd['tagged_entities'].append([local_act[0],local_act[-1],str(paragraph[local_act[0]:local_act[-1]+1]),\"Act\"])\n",
        "        local_act = list()\n",
        "      if count_cite == True:\n",
        "        tupleAdd['tagged_entities'].append([local_cite[0],local_cite[-1],str(paragraph[local_cite[0]:local_cite[-1]+1]),\"Cite\"])\n",
        "        local_cite = list()\n",
        "\n",
        "      count_section = False\n",
        "      count_act = False\n",
        "      count_case = False\n",
        "      count_cite = False\n",
        "  \n",
        "      if count_article == False:\n",
        "        local_article.append(int(combined[i][0]))\n",
        "        local_article.append(int(combined[i][1]))\n",
        "        count_article = True\n",
        "      else:\n",
        "        local_article.append(int(combined[i][1]))\n",
        "\n",
        "    # section check\n",
        "    elif ner_tags[i].find(\"Section\") != -1:\n",
        "      if count_article == True:\n",
        "        tupleAdd['tagged_entities'].append([local_article[0],local_article[-1],str(paragraph[local_article[0]:local_article[-1]+1]),\"Article\"])\n",
        "        local_article = list()\n",
        "      if count_case == True:\n",
        "        tupleAdd['tagged_entities'].append([local_case[0],local_case[-1],str(paragraph[local_case[0]:local_case[-1]+1]),\"Case\"])\n",
        "        local_case = list()\n",
        "      if count_act == True:\n",
        "        tupleAdd['tagged_entities'].append([local_act[0],local_act[-1],str(paragraph[local_act[0]:local_act[-1]+1]),\"Act\"])\n",
        "        local_act = list()\n",
        "      if count_cite == True:\n",
        "        tupleAdd['tagged_entities'].append([local_cite[0],local_cite[-1],str(paragraph[local_cite[0]:local_cite[-1]+1]),\"Cite\"])\n",
        "        local_cite = list()\n",
        "\n",
        "      count_article = False\n",
        "      count_act = False\n",
        "      count_case = False\n",
        "      count_cite = False\n",
        "\n",
        "      if count_section == False:\n",
        "        local_section.append(int(combined[i][0]))\n",
        "        local_section.append(int(combined[i][1]))\n",
        "        count_section = True\n",
        "      else:\n",
        "        local_section.append(int(combined[i][1]))\n",
        "\n",
        "    # case check\n",
        "    elif ner_tags[i].find(\"Case\") != -1:\n",
        "      if count_article == True:\n",
        "        tupleAdd['tagged_entities'].append([local_article[0],local_article[-1],str(paragraph[local_article[0]:local_article[-1]+1]),\"Article\"])\n",
        "        local_list = list()\n",
        "      if count_act == True:\n",
        "        tupleAdd['tagged_entities'].append([local_act[0],local_act[-1],str(paragraph[local_act[0]:local_act[-1]+1]),\"Act\"])\n",
        "        local_act = list()\n",
        "      if count_section == True:\n",
        "        tupleAdd['tagged_entities'].append([local_section[0],local_section[-1],str(paragraph[local_section[0]:local_section[-1]+1]),\"Section\"])\n",
        "        local_section = list()\n",
        "      if count_cite == True:\n",
        "        tupleAdd['tagged_entities'].append([local_cite[0],local_cite[-1],str(paragraph[local_cite[0]:local_cite[-1]+1]),\"Cite\"])\n",
        "        local_cite = list()\n",
        "      \n",
        "      count_article = False\n",
        "      count_act = False\n",
        "      count_section = False\n",
        "      count_cite = False\n",
        "\n",
        "      if count_case == False:\n",
        "        local_case.append(int(combined[i][0]))\n",
        "        local_case.append(int(combined[i][1]))\n",
        "        count_case = True\n",
        "      else:\n",
        "        local_case.append(int(combined[i][1]))\n",
        "\n",
        "    # act check\n",
        "    elif ner_tags[i].find(\"Act\") != -1:\n",
        "      if count_section == True:\n",
        "        tupleAdd['tagged_entities'].append([local_section[0],local_section[-1],str(paragraph[local_section[0]:local_section[-1]+1]),\"Section\"])\n",
        "        local_section = list()\n",
        "      if count_article == True:\n",
        "        tupleAdd['tagged_entities'].append([local_article[0],local_article[-1],str(paragraph[local_article[0]:local_article[-1]+1]),\"Article\"])\n",
        "        local_article = list()\n",
        "      if count_case == True:\n",
        "        tupleAdd['tagged_entities'].append([local_case[0],local_case[-1],str(paragraph[local_case[0]:local_case[-1]+1]),\"Case\"])\n",
        "        local_case = list()\n",
        "      if count_cite == True:\n",
        "        tupleAdd['tagged_entities'].append([local_cite[0],local_cite[-1],str(paragraph[local_cite[0]:local_cite[-1]+1]),\"Cite\"])\n",
        "        local_cite = list()\n",
        "\n",
        "      count_article = False\n",
        "      count_case = False\n",
        "      count_section = False\n",
        "      count_cite = False\n",
        "      \n",
        "      if count_act == False:\n",
        "        local_act.append(int(combined[i][0]))\n",
        "        local_act.append(int(combined[i][1]))\n",
        "        count_act = True\n",
        "      else:\n",
        "        local_act.append(int(combined[i][1]))    \n",
        "\n",
        "    # cite check    \n",
        "    elif ner_tags[i].find(\"Cite\") != -1:\n",
        "      if count_article == True:\n",
        "        tupleAdd['tagged_entities'].append([local_article[0],local_article[-1],str(paragraph[local_article[0]:local_article[-1]+1]),\"Article\"])\n",
        "        local_article = list()\n",
        "      if count_case == True:\n",
        "        tupleAdd['tagged_entities'].append([local_case[0],local_case[-1],str(paragraph[local_case[0]:local_case[-1]+1]),\"Case\"])\n",
        "        local_case = list()\n",
        "      if count_act == True:\n",
        "        tupleAdd['tagged_entities'].append([local_act[0],local_act[-1],str(paragraph[local_act[0]:local_act[-1]+1]),\"Act\"])\n",
        "        local_act = list()\n",
        "      if count_section == True:\n",
        "        tupleAdd['tagged_entities'].append([local_section[0],local_section[-1],str(paragraph[local_section[0]:local_section[-1]+1]),\"Section\"])\n",
        "        local_section = list()\n",
        "\n",
        "      count_article = False\n",
        "      count_act = False\n",
        "      count_case = False\n",
        "      count_section = False\n",
        "\n",
        "      if count_cite == False:\n",
        "        local_cite.append(int(combined[i][0]))\n",
        "        local_cite.append(int(combined[i][1]))\n",
        "        count_cite = True\n",
        "      else:\n",
        "        local_cite.append(int(combined[i][1]))\n",
        "\n",
        "  return tupleAdd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUTGJ0qp5Sc4"
      },
      "source": [
        "def modelPredict(sent): # takes one sentence and gives predictions. \n",
        "  \"\"\"\n",
        "  args: sentence (one entire parapraph including subquotes and blockquotes in our case)\n",
        "  returns: dataframe consisting of columns: 'sent_id','token','start','end','token2','ner' (note: ignore token2 column)\n",
        "  \"\"\"\n",
        "  model2 = ner_prediction_pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))\n",
        "  detailed_result = LightPipeline(model2).fullAnnotate(sent)\n",
        "  tuples = []\n",
        "\n",
        "  for x,y,z in zip(detailed_result[0][\"token\"], detailed_result[0][\"bert\"], detailed_result[0][\"ner\"]):\n",
        "\n",
        "    tuples.append((int(x.metadata['sentence']), x.result, x.begin, x.end, y.result, z.result))\n",
        "\n",
        "  predicted_df = pd.DataFrame(tuples, columns=['sent_id','token','start','end','token2','ner'])\n",
        " \n",
        "  return predicted_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF6zGB5grpeO"
      },
      "source": [
        "def appendToJson(df,json_data,para_id, all_together): # appends to the json\n",
        "  \"\"\"\n",
        "  args: df - dataFrame as returned by modelPredict, json_data - dictionary - the json data to update,\n",
        "        para_id - int - current para_id in the iteration - to update paragraphs at this index in json_data,\n",
        "        all_together - string - string - consists of the current para, subPara, and blockQuotes\n",
        "  returns: the updated json data with tagged entities - dict\n",
        "  note: this method calls makeTuple method to get the final tagged entities dictionary\n",
        "  \"\"\"\n",
        "  tupleAdd = {\"tagged_entities\":[]}  # the tuple to be added to the final json\n",
        "  starts = list(df['start'])\n",
        "  ends = list(df['end'])\n",
        "  ner_tags = list(df['ner'])\n",
        "  tokens = list(df['token'])\n",
        "  combined = []  # [start index of each token, end index of each token]\n",
        "  \n",
        "  for i in range(len(starts)): \n",
        "    combined.append([starts[i],ends[i]])\n",
        "\n",
        "  final_add = makeTuple(ner_tags,combined,tupleAdd, all_together) \n",
        "  print(\"to be added\",final_add)\n",
        "\n",
        "  if(len(final_add['tagged_entities']) != 0):\n",
        "    json_data['paragraphs'][para_id-1].update(final_add)\n",
        "    return json_data\n",
        "  else:\n",
        "    tupleAdd = {\"tagged_entities\":[]}\n",
        "    json_data['paragraphs'][para_id-1].update(final_add)\n",
        "    return json_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIiixypIqehz"
      },
      "source": [
        "def mainCall(input_csv_path,json_file_path): # main function to call other functions\n",
        "  \"\"\"\n",
        "  args: input_csv - path - csv columns - ['paraID', 'contentID', 'Paragraph']\n",
        "        json_file_path - path to json file - need to update this json\n",
        "  returns: the updated json file\n",
        "  note: this method calls modelPredict method and appendToJson method\n",
        "  \"\"\"\n",
        "  input_csv = pd.read_csv(input_csv_path)\n",
        "  input_csv.columns = ['ParaID','ContentID','Paragraph']\n",
        "  json_data = dict()\n",
        "\n",
        "  with open(json_file_path, 'r') as j:\n",
        "    json_data = json.loads(j.read())\n",
        " \n",
        "  content_id = list(input_csv['ContentID'])\n",
        "  paragraphs = list(input_csv['Paragraph']) # main content\n",
        "\n",
        "  updated_json = open('updated_json.json','w')  # final json to return \n",
        "  df = pd.DataFrame()\n",
        "  \n",
        "  # iteration counters\n",
        "  content_counter = 0\n",
        "  sent_count = 0\n",
        "  k = 0\n",
        "  last_counter = 0\n",
        "  updated_data = \"\" \n",
        "  all_together = \"\" # string to append a particular para id's para, subPara, and blockQuotes\n",
        "\n",
        "  prev_para_id = list(input_csv['ParaID'])[0]\n",
        "\n",
        "  for para_id in list(input_csv['ParaID']): # goes through all the para_id(s) in the json paragraphs\n",
        "    if para_id == prev_para_id:\n",
        "\n",
        "      print(\"para_id\", para_id)\n",
        "      print(\"content_id\", content_id[content_counter])\n",
        "      \n",
        "      all_together += paragraphs[k]\n",
        "      print(\"checking for last\",input_csv['ParaID'].iloc[-1] == para_id)\n",
        "      if (input_csv['ParaID'].iloc[-1] == para_id):\n",
        "        if content_counter >= len(paragraphs)-1:\n",
        "          print(\"para_id to send\", para_id)\n",
        "          df = df.append(modelPredict(all_together)) # calling modelPredict\n",
        "          print(\"all_together\",all_together)\n",
        "          updated_data = appendToJson(df,json_data,para_id, all_together)  # calling appendToJson\n",
        "          all_together = \" \"\n",
        "          break      \n",
        "      k += 1\n",
        "      \n",
        "    else:\n",
        "      print(\"para_id to send\", prev_para_id)\n",
        "      df = df.append(modelPredict(all_together)) # calling modelPredict\n",
        "      \n",
        "      print(\"all_together\",all_together)\n",
        "      updated_data = appendToJson(df,json_data,prev_para_id, all_together)  # calling appendToJson \n",
        "      all_together = \" \"\n",
        "\n",
        "      df = pd.DataFrame()\n",
        "      print(\"else\")\n",
        "\n",
        "      if list(input_csv['ParaID'])[(list(input_csv['ParaID'])).index(para_id)] == para_id:\n",
        "        print(\"changed para_id\",para_id) \n",
        "\n",
        "        print(\"content_id\", content_id[content_counter])\n",
        "        all_together += paragraphs[k]\n",
        "        k += 1\n",
        "      if input_csv['ParaID'].iloc[-1] == para_id:\n",
        "\n",
        "        if content_counter >= len(paragraphs)-1:\n",
        "          print(\"para_id to send\", para_id)\n",
        "          df = df.append(modelPredict(all_together)) # calling modelPredict\n",
        "          print(\"all_together\",all_together)\n",
        "          updated_data = appendToJson(df,json_data,para_id, all_together)  # calling appendToJson\n",
        "          all_together = \" \"\n",
        "          break      \n",
        "\n",
        "    prev_para_id = para_id\n",
        "    content_counter += 1\n",
        "    print(\"___________________________\")\n",
        "  json.dump(updated_data, updated_json)\n",
        "  print(\"success!\")\n",
        "  updated_json.close()\n",
        "  return updated_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOSBy02OFczg"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFVO3LPRPI0G",
        "outputId": "ccfb0909-5990-43e4-cbe9-2ca85a7753ea"
      },
      "source": [
        "# print(mainCall('/content/5_sp.csv','/content/5_sp.json'))\n",
        "# print(mainCall('/content/3_bq.csv','/content/3_bq.json'))\n",
        "# print(mainCall('/content/4_multiple_bq.csv','/content/4_multiple_bq.json'))\n",
        "# print(mainCall('/content/5.csv','/content/5.json'))\n",
        "# print(mainCall('/content/4.csv','/content/4.json'))\n",
        "# print(mainCall('/content/3.csv','/content/3.json'))\n",
        "# print(mainCall('/content/2.csv','/content/2.json'))\n",
        "# print(mainCall('/content/1.csv','/content/1.json'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "para_id 1\n",
            "content_id p_1\n",
            "checking for last False\n",
            "___________________________\n",
            "para_id to send 1\n",
            "all_together Performance of  judicial duty in  the  manner  prescribed  by  law  isfundamental to the concept of rule of law in  a  democratic  State.  It  hasbeen quite often said and, rightly so, that the judiciary is  the  protectorand preserver of rule of law.  Effective functioning of the said  sacrosanctduty has been entrusted to the judiciary and that  entrustment  expects  thecourts to conduct the judicial  proceeding  with  dignity,  objectivity  andrationality and finally determine the same in accordance  with  law.  Errorsare bound to occur but there cannot  be  deliberate  peccability  which  cannever be countenanced.   The  plinth  of  justice   dispensation  system  isfounded on the faith, trust and confidence of the people and nothing can  beallowed to contaminate and corrode the same.  A  litigant  who  comes  to  acourt of law expects that inherent and essential principles of  adjudicationlike adherence to doctrine of  audi  alteram  partem,  rules  pertaining  tofundamental adjective and seminal substantive  law  shall  be  followed  andultimately there shall be a reasoned verdict.   When  the  accused  faces  acharge in a court of  law,  he  expects  a  fair  trial.  The  victim  whosegrievance and agony have given rise to the trial also expects  that  justiceshould be done in accordance with law. Thus,  a  fair  trial  leading  to  ajudgment is necessitous in law and that is the assurance that is thought  ofon both sides.  The exponent on behalf of the accused  cannot  be  permittedto command the trial as desired by his philosophy of trial on  the  plea  offair trial and similarly, the proponent on behalf of the victim  should  notalways be allowed to ventilate the grievance that his  cause  has  not  beenfairly dealt with in the name of  fair  trial.  Therefore,  the  concept  ofexpediency and fair trial is quite applicable to the accused as well  as  tothe victim.  The result of such trial is to end in a  judgment  as  requiredto be pronounced in accordance with law. And, that is how the  stability  ofthe creditability in the institution is maintained.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 2\n",
            "content_id p_2\n",
            "___________________________\n",
            "para_id to send 2\n",
            "all_together  2.       The above prefatory note has relevance, a significant one,  to  thecase at hand. To appreciate the controversy, certain facts are requisite  tobe noted.  The marriage between the appellant No.  1  and  Ruby  Singh,  thedeceased, was  solemnized  according  to  Hindu  rites  on  22.06.1997.  Shecommitted suicide at her matrimonial home on 01.12.1998.   Kameshwar  Prataplodged FIR No. 194/98 at Police Station Lakhanpur,  Distt.  Sarguja  againstAjay  Singh  (husband),  Sureshwar  Singh  (father-in-law),  Dhanwanti  Devi(mother-in-law) and Kiran  Singh  (sister-in-law)  for  offences  punishableunder Section 304B, 34 of the Indian Penal Code (IPC)  and  other  offences.After the criminal  law  was  set  in  motion,  investigating  agency  aftercommencement of investigation  and  after  completion  thereof  laid  chargesheet under Sections 304B, 498A/34, 328 IPC read with Section 3/4  of  DowryProhibition Act, 1961 against the accused persons before the Court of  ChiefJudicial Magistrate, Ambikapur, who, in turn, committed the  matter  to  theCourt of Session and eventually the matter was tried  by  Second  AdditionalSessions Judge, Ambikapur. We are, in the present case, not  concerned  withhow many witnesses were examined  by  the  trial  court  or  how  the  trialcontinued.  What needs to be stated is that the learned trial  Judge  passedan order in the order sheet that recorded that the accused persons had  beenacquitted as per the judgment separately typed, signed and dated.\n",
            "to be added {'tagged_entities': [[615, 626, 'Section 304B', 'Section'], [629, 630, '34', 'Section'], [639, 655, 'Indian Penal Code', 'Act'], [658, 660, 'IPC', 'Act'], [849, 861, 'Sections 304B', 'Section'], [864, 870, '498A/34', 'Section'], [877, 879, 'IPC', 'Act'], [891, 901, 'Section 3/4', 'Section'], [908, 933, 'DowryProhibition Act, 1961', 'Act']]}\n",
            "else\n",
            "changed para_id 3\n",
            "content_id p_3\n",
            "___________________________\n",
            "para_id to send 3\n",
            "all_together  3.    A member of the State Bar Council sent a complaint to the Registry  ofthe High Court of Chhattisgarh, Bilaspur alleging that learned  trial  judgehad acquitted the accused persons but no judgment had  been  rendered.   TheRegistrar (Vigilance) of the High Court issued a memorandum to the  Districtand Sessions Judge, Surguja at Ambikapur on 18.02.2008 to inquire  into  thematter and submit a  report.  The  concerned  District  and  Sessions  Judgesubmitted the report to the High Court on the  same  date  stating  that  nojudgments were found in the records of such cases. It has also been  broughtto the notice of the High Court  that  in  sessions  trials  being  SessionsTrial No. 148 of 1999 and Sessions Trial No. 71  of  1995  though  the  sametrial judge had purportedly  delivered  the  judgments  but  they  were  notavailable on record as the judgments had not actually been  dictated,  datedor signed.  Thereafter the matter was placed before the Full  Court  of  theHigh Court on 04.03.2008 on which date a resolution was passed  placing  theconcerned trial judge under suspension in contemplation  of  a  departmentalinquiry.  At the same time, the Full Court took  the  decision  to  transferthe cases in question  from  the  concerned  trial  judge  to  the  file  ofDistrict  and  Sessions  Judge,  Surguja  at  Ambikapur  for  rehearing  anddisposal. It is worthy to note here  that  the  concerned  officer  was  putunder suspension and after  completion  of  inquiry  was  imposed  with  thepunishment of compulsory retirement on 22.03.2011. We make it clear that  weare not concerned with the said punishment in the case.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 4\n",
            "content_id p_4\n",
            "___________________________\n",
            "para_id to send 4\n",
            "all_together  4.    After the decision was taken for transferring the cases  by  the  FullCourt for rehearing, three writ petitions  forming  the  subject  matter  ofWrit Petition (Criminal) Nos. 2796 of 2008, 2238 of 2008  and  276  of  2010were filed. The accused in  Sessions  Trial  No.  148  of  1999  filed  WritPetition (Criminal) Nos. 2796 of 2008  and  2238  of  2008  and  accused  inSessions Trial  No. 71 of 1995 filed the other writ petition, that is,  WritPetition (Criminal) No. 276 of 2010.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 5\n",
            "content_id p_5\n",
            "___________________________\n",
            "para_id to send 5\n",
            "all_together  5.    The controversy really centers around two issues, namely, whether  thelearned trial judge had really  pronounced  the  judgment  of  acquittal  on31.10.2007 and whether  the  High  Court  could  have  in  exercise  of  itsadministrative power treated the trial as pending and transferred  the  samefrom the Court of Second Additional Sessions Judge, Ambikapur to  the  Courtof District and Sessions Judge,  Surguja  at  Ambikapur  for  rehearing  anddisposal.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 6\n",
            "content_id p_6\n",
            "___________________________\n",
            "para_id to send 6\n",
            "all_together  6.    It is urged by learned counsel for the appellants that the  nature  oforder passed by the learned trial judge would amount to a  judgment  and  inthe absence of any appeal preferred by the State there could not  have  beena direction for rehearing of the sessions case as such action runs  contraryto the provisions of CrPC.  Learned  counsel  would  submit  that  the  HighCourt  in  exercise  of  power  of  the  superintendence  could   not   havetransferred the case treating it as pending on its administrative side.   Tobolster the said submission he  has  placed  reliance  on  Ouseph  Mathai  &others v. M. Abdul Khadir[1], Essen Deinki v. Rajiv Kumar[2] and  Surya  DevRai v. Ram Chander Rai and others[3].\n",
            "to be added {'tagged_entities': [[326, 329, 'CrPC', 'Act'], [652, 653, 'v.', 'Case'], [689, 689, 'v', 'Case']]}\n",
            "else\n",
            "changed para_id 7\n",
            "content_id p_7\n",
            "___________________________\n",
            "para_id to send 7\n",
            "all_together  7.    Mr. C.D. Singh, learned counsel  for  the  State  submitted  that  theapproach of the High Court is absolutely infallible  and  does  not  warrantany interference by this Court.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 8\n",
            "content_id p_8\n",
            "___________________________\n",
            "para_id to send 8\n",
            "all_together  8.    To appreciate the controversy, it is necessary to refer to  the  ordersheet in Sessions Trial No. 71 of 1995. The trial  judge  on  28.1.2008  hadpassed the following order:-\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 9\n",
            "content_id p_9\n",
            "___________________________\n",
            "para_id to send 9\n",
            "all_together  28.1.2008:\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 10\n",
            "content_id p_10\n",
            "___________________________\n",
            "para_id to send 10\n",
            "all_together  State represented by Shri Rajesh Tiwari, A.G.P.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 11\n",
            "content_id p_11\n",
            "___________________________\n",
            "para_id to send 11\n",
            "all_together  Accused along with their Counsel Shri Arvind Mehta, AdvocateThe judgment has been typed separately.  The same  has  been  dated,  signedand announced.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 12\n",
            "content_id p_12\n",
            "___________________________\n",
            "para_id to send 12\n",
            "all_together  Resultantly, Accused T.P. Ratre is acquitted of  the  charge  under  Section306 IPC.\n",
            "to be added {'tagged_entities': [[70, 79, 'Section306', 'Section'], [81, 83, 'IPC', 'Act']]}\n",
            "else\n",
            "changed para_id 13\n",
            "content_id p_13\n",
            "___________________________\n",
            "para_id to send 13\n",
            "all_together  A copy of  this  judgment  be  sent  to  the  District  Magistrate,  Surguja(Ambikapur) through A.G.P.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 14\n",
            "content_id p_14\n",
            "___________________________\n",
            "para_id to send 14\n",
            "all_together  Proceedings completed.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 15\n",
            "content_id p_15\n",
            "___________________________\n",
            "para_id to send 15\n",
            "all_together  The result be noted in the register and the record be  sent  to  the  RecordRoom.Be it noted, in the other Sessions Trial, i.e., Sessions Trial  No.  148  of1999 almost similar order has been passed. Be  it  stated,  apart  from  theaforesaid order, as per  the  enquiry  conducted  by  the  learned  DistrictJudge, there was nothing on record.  The trial judge had  not  dictated  theorder in open court.  In such a situation, it is to  be  determined  whetherthe judgment had been delivered by the trial judge or not.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 16\n",
            "content_id p_16\n",
            "___________________________\n",
            "para_id to send 16\n",
            "all_together  9.    Chapter XVIII of CrPC provides for trial before a  court  of  session.Section 227 empowers the trial judge to discharge the accused after  hearingthe submissions of the accused and the prosecution and  on  being  satisfiedthat there is no sufficient ground for proceeding against the accused.   Thekey words of the Section are not sufficient ground for  proceeding  againstthe   accused.   Interpreting   the   said   provision,   the   Court    inP. Vijayan v. State of Kerala and another[4] has held that the Judge is  nota mere post office to frame the charge at the  behest  of  the  prosecution,but has to exercise his judicial mind to the facts of the case in  order  todetermine whether a case for trial has been made out by the prosecution.  Inassessing this fact, it is not necessary for the court  to  enter  into  thepros and cons of the matter or into a weighing  and  balancing  of  evidenceand probabilities which is really the  function  of  the  court,  after  thetrial starts. At the stage of Section 227, the Judge has merely to sift  theevidence in order to find out whether or not there is sufficient ground  forproceeding against the accused. In other words, the  sufficiency  of  groundwould take within its fold the  nature  of  the  evidence  recorded  by  thepolice or the documents produced before the court which  ex  facie  disclosethat there are suspicious circumstances against the accused so as  to  framea charge against him.\n",
            "to be added {'tagged_entities': [[24, 27, 'CrPC', 'Act'], [468, 468, 'v', 'Case'], [1019, 1029, 'Section 227', 'Section']]}\n",
            "else\n",
            "changed para_id 17\n",
            "content_id p_17\n",
            "___________________________\n",
            "para_id to send 17\n",
            "all_together  10.   Section 228 empowers the trial judge to  frame  the  charge.   Section229 provides if the accused pleads guilty, the Judge shall record  the  pleaand may, in his discretion, convict him thereon.  Section 230  provides  fordate for prosecution evidence.  Section 231  deals  with  the  evidence  forprosecution.  Section 232 provides that if, after taking  the  evidence  forthe prosecution, examining the  accused  and  hearing  the  prosecution  thedefence on the point, the Judge considers that there  is  no  evidence  thatthe accused committed the offence,  the  Judge  shall  record  an  order  ofacquittal. Section 233 stipulates that where the accused  is  not  acquittedunder Section 232 he shall be called  upon  to  enter  on  his  defence  andadduce any evidence he may have in support thereof.   Section  234  providesfor arguments. Section 235 which  provides  for  judgment  of  acquittal  orconviction reads as follows:-\n",
            "to be added {'tagged_entities': [[7, 17, 'Section 228', 'Section'], [70, 79, 'Section229', 'Section'], [203, 213, 'Section 230', 'Section'], [261, 271, 'Section 231', 'Section'], [319, 329, 'Section 232', 'Section'], [620, 630, 'Section 233', 'Section'], [691, 701, 'Section 232', 'Section'], [815, 826, 'Section  234', 'Section'], [852, 862, 'Section 235', 'Section']]}\n",
            "else\n",
            "changed para_id 18\n",
            "content_id p_18\n",
            "___________________________\n",
            "para_id to send 18\n",
            "all_together  235. Judgment of acquittal or conviction.   (1)  After  hearing  argumentsand points of law (if any), the Judge shall give a judgment in the case.(2) If the accused is convicted, the Judge  shall,  unless  he  proceeds  inaccordance with the provisions of section  360,  hear  the  accused  on  thequestion of sentence, and then pass sentence on him according to law.\n",
            "to be added {'tagged_entities': [[259, 270, 'section  360', 'Section']]}\n",
            "else\n",
            "changed para_id 19\n",
            "content_id p_19\n",
            "___________________________\n",
            "para_id to send 19\n",
            "all_together  11.   Chapter XXIV provides for  general  provisions  as  to  inquiries  andtrials. Chapter XXVII deals with the judgment. Section  353  lays  down  theprocedure for pronouncement of the judgment. The  said  provision  reads  asfollows:-\n",
            "to be added {'tagged_entities': [[124, 135, 'Section  353', 'Section']]}\n",
            "else\n",
            "changed para_id 20\n",
            "content_id p_20\n",
            "___________________________\n",
            "para_id to send 20\n",
            "all_together  353. Judgment -\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 21\n",
            "content_id p_21\n",
            "___________________________\n",
            "para_id to send 21\n",
            "all_together  (1)The  judgment  in  every  trial  in  any  Criminal  Court  of   originaljurisdiction shall be pronounced in open  Court  by  the  presiding  officerimmediately after the termination of the trial or at  some  subsequent  timeof which notice shall be given to the parties or their pleaders,-\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 22\n",
            "content_id p_22\n",
            "___________________________\n",
            "para_id to send 22\n",
            "all_together  (a)by delivering the whole of the judgment; or\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 23\n",
            "content_id p_23\n",
            "___________________________\n",
            "para_id to send 23\n",
            "all_together  (b)by reading out the whole of the judgment; or\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 24\n",
            "content_id p_24\n",
            "___________________________\n",
            "para_id to send 24\n",
            "all_together  (c)by reading out the operative part of the  judgment  and  explaining  thesubstance of the judgment in a language which is understood by  the  accusedor his pleader.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 25\n",
            "content_id p_25\n",
            "___________________________\n",
            "para_id to send 25\n",
            "all_together  (2)Where the judgment is delivered under clause  (a)  of  sub-section  (1),the presiding officer shall cause it to be taken down  in  short-hand,  signthe transcript and every page thereof as soon  as  it  is  made  ready,  andwrite on it the date of the delivery of the judgment in open Court.(3)Where the judgment or the operative  part  thereof  is  read  out  underclause (b) or clause (c) of sub- section (1), as the case may be,  it  shallbe dated and signed by the presiding officer in open Court,  and  if  it  isnot written with his own hand, every page of the judgment  shall  be  signedby him.\n",
            "to be added {'tagged_entities': [[405, 411, 'section', 'Section']]}\n",
            "else\n",
            "changed para_id 26\n",
            "content_id p_26\n",
            "___________________________\n",
            "para_id to send 26\n",
            "all_together  (4)Where the judgment is pronounced in the manner specified in  clause  (c)of  sub-section  (1),  the  whole  judgment  or  a  copy  thereof  shall  beimmediately made available for the perusal of the parties or their  pleadersfree of cost.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 27\n",
            "content_id p_27\n",
            "___________________________\n",
            "para_id to send 27\n",
            "all_together  (5)If the accused is in custody,  he  shall  be  brought  up  to  hear  thejudgment pronounced.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 28\n",
            "content_id p_28\n",
            "___________________________\n",
            "para_id to send 28\n",
            "all_together  (6)If the accused is not in custody, he shall be required by the  Court  toattend  to  hear  the  judgment  pronounced,  except  where   his   personalattendance during the trial has been dispensed with and the sentence is  oneof fine only or he  is  acquitted:  Provided  that,  where  there  are  moreaccused than one, and one or more of them do not attend  the  Court  on  thedate on which the judgment is to be pronounced, the presiding  officer  may,in order to avoid undue delay in the disposal of  the  case,  pronounce  thejudgment notwithstanding their absence.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 29\n",
            "content_id p_29\n",
            "___________________________\n",
            "para_id to send 29\n",
            "all_together  (7)No judgment delivered by any  Criminal  Court  shall  be  deemed  to  beinvalid by reason only of the absence of any party or  his  pleader  on  theday or from the place notified for the delivery thereof, or of any  omissionto serve, or defect in serving, on the parties or their pleaders, or any  ofthem, the notice of such day and place.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 30\n",
            "content_id p_30\n",
            "___________________________\n",
            "para_id to send 30\n",
            "all_together  (8)Nothing in this section shall be construed  to  limit  in  any  way  theextent of the provisions of section 465.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 31\n",
            "content_id p_31\n",
            "___________________________\n",
            "para_id to send 31\n",
            "all_together  12.   Section 354 provides for language and contents of  the  judgment.  Thesaid provision reads as follows:-\n",
            "to be added {'tagged_entities': [[7, 17, 'Section 354', 'Section']]}\n",
            "else\n",
            "changed para_id 32\n",
            "content_id p_32\n",
            "___________________________\n",
            "para_id to send 32\n",
            "all_together  354. Language and contents of judgment.-\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 33\n",
            "content_id p_33\n",
            "___________________________\n",
            "para_id to send 33\n",
            "all_together  (1)Except as otherwise expressly provided  by  this  Code,  every  judgmentreferred to in section 353,-\n",
            "to be added {'tagged_entities': [[92, 102, 'section 353', 'Section']]}\n",
            "else\n",
            "changed para_id 34\n",
            "content_id p_34\n",
            "___________________________\n",
            "para_id to send 34\n",
            "all_together  (a)shall be written in the language of the Court;\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 35\n",
            "content_id p_35\n",
            "___________________________\n",
            "para_id to send 35\n",
            "all_together  (b)shall contain the  point  or  points  for  determination,  the  decisionthereon and the reasons for the decision;\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 36\n",
            "content_id p_36\n",
            "___________________________\n",
            "para_id to send 36\n",
            "all_together  (c)shall specify the offence (if any) of which,  and  the  section  of  theIndian Penal Code (45 of 1860 ) or other law under  which,  the  accused  isconvicted and the punishment to which he is sentenced;\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 37\n",
            "content_id p_37\n",
            "___________________________\n",
            "para_id to send 37\n",
            "all_together  (d)if it be a judgment of acquittal, shall state the offence of  which  theaccused is acquitted and direct that he be set at liberty.(2)When the conviction is under the Indian Penal Code (45 of  1860  ),  andit is doubtful under which of two sections, or under which of two  parts  ofthe  same  section,  of  that  Code  the  offence  falls,  the  Court  shalldistinctly express the same, and pass judgment in the alternative.(3)When the conviction is for an offence punishable with death or,  in  thealternative, with imprisonment for  life  or  imprisonment  for  a  term  ofyears, the judgment shall state the reasons for the sentence  awarded,  and,in the case of sentence of death, the special reasons for such sentence.(4)When the conviction is for an offence punishable with  imprisonment  fora  term  of  one  year  or  more,  but  the  Court  imposes  a  sentence  ofimprisonment for a term of less than  three  months,  it  shall  record  itsreasons  for  awarding  such  sentence,  unless  the  sentence  is  one   ofimprisonment till the rising of the Court  or  unless  the  case  was  triedsummarily under the provisions of this Code.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 38\n",
            "content_id p_38\n",
            "___________________________\n",
            "para_id to send 38\n",
            "all_together  (5)When any person is sentenced to death, the sentence  shall  direct  thathe be hanged by the neck till he is dead.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 39\n",
            "content_id p_39\n",
            "___________________________\n",
            "para_id to send 39\n",
            "all_together  (6)Every order under section 117 or sub-section  (2)  of  section  138  andevery final order made under section 125, section 145 or section  147  shallcontain the point or points for determination, the decision thereon and  thereasons for the decision.\n",
            "to be added {'tagged_entities': [[23, 33, 'section 117', 'Section'], [60, 71, 'section  138', 'Section'], [106, 116, 'section 125', 'Section'], [119, 129, 'section 145', 'Section'], [134, 145, 'section  147', 'Section']]}\n",
            "else\n",
            "changed para_id 40\n",
            "content_id p_40\n",
            "___________________________\n",
            "para_id to send 40\n",
            "all_together  13.   Section 362 has the heading Court not to alter  judgment.  The  saidprovision is as follows:-\n",
            "to be added {'tagged_entities': [[7, 17, 'Section 362', 'Section']]}\n",
            "else\n",
            "changed para_id 41\n",
            "content_id p_41\n",
            "___________________________\n",
            "para_id to send 41\n",
            "all_together  362. Court not to alter judgment.?Save as otherwise provided by  this  Codeor by any other law for the time being in  force,  no  Court,  when  it  hassigned its judgment or final order disposing  of  a  case,  shall  alter  orreview the same except to correct a clerical or arithmetical error.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 42\n",
            "content_id p_42\n",
            "___________________________\n",
            "para_id to send 42\n",
            "all_together  14.   Interpreting  the  said  provision  in  the  context  of  exercise  ofinherent power of the High Court under Section 482 CrPC this Court  in  Smt.Sooraj Devi v. Pyare Lal and another[5] held thus:-\n",
            "to be added {'tagged_entities': [[116, 126, 'Section 482', 'Section'], [128, 131, 'CrPC', 'Act'], [165, 165, 'v', 'Case']]}\n",
            "else\n",
            "changed para_id 43\n",
            "content_id p_43\n",
            "___________________________\n",
            "para_id to send 43\n",
            "all_together  5. The appellant points out that he invoked the inherent power of the  HighCourt saved by  Section  482  of  the  Code  and  that  notwithstanding  theprohibition imposed by Section  362  the  High  Court  had  power  to  grantrelief. Now it is well settled that the inherent power of the  court  cannotbe exercised for doing that which is specifically  prohibited  by  the  Code(Sankatha Singh v. State of U.P.[6]). It is true  that  the  prohibition  inSection 362 against the court altering or reviewing its judgment is  subjectto what is otherwise provided by this Court or by any  other  law  for  thetime being in force. Those words, however, refer to those  provisions  onlywhere the court has been expressly authorised by the Code or  other  law  toalter or review its judgment.  The  inherent  power  of  the  court  is  notcontemplated  by  the  saving  provision  contained  in  Section  362   and,therefore, the attempt to invoke that power can be of no avail.We have referred to the aforesaid  decision  to  illustrate  that  the  CrPCconfers absolute sanctity to the judgment once it is  pronounced.   It  doesnot conceive of any kind of alteration.\n",
            "to be added {'tagged_entities': [[93, 104, 'Section  482', 'Section'], [176, 187, 'Section  362', 'Section'], [894, 905, 'Section  362', 'Section']]}\n",
            "else\n",
            "changed para_id 44\n",
            "content_id p_44\n",
            "___________________________\n",
            "para_id to send 44\n",
            "all_together  15.   Section 363 provides copy of judgment to be given to the  accused  andother persons.  Section 364 provides for the situation  where  the  judgmentrequires to be translated.\n",
            "to be added {'tagged_entities': [[7, 17, 'Section 363', 'Section'], [93, 103, 'Section 364', 'Section']]}\n",
            "else\n",
            "changed para_id 45\n",
            "content_id p_45\n",
            "___________________________\n",
            "para_id to send 45\n",
            "all_together  16.   It is apposite to note that though  CrPC  does  not  define  the  termjudgment, yet it  has  clearly  laid  down  how  the  judgment  is  to  bepronounced. The provisions clearly spell out that it is  imperative  on  thepart of the learned trial judge to pronounce the judgment in open  court  bydelivering the whole of the judgment or by reading  out  the  whole  of  thejudgment  or  by  reading  out  the  operative  part  of  the  judgment  andexplaining the substance of the judgment in a language which  is  understoodby the accused or his pleader.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 46\n",
            "content_id p_46\n",
            "___________________________\n",
            "para_id to send 46\n",
            "all_together  17.   We have already noted that the  judgment  was  not  dictated  in  opencourt.  Code of Criminal Procedure provides reading of  the  operative  partof the judgment. It means that the trial judge may not  read  the  whole  ofthe judgment and may read operative part of the judgment but it does not  inany way suggest that the result of  the  case  will  be  announced  and  thejudgment would not be available on record. Non-  availability  of  judgment,needless to say, can never be a judgment because there is no declaration  byway of pronouncement in the open court that the accused has  been  convictedor acquitted.  A judgment, as has been always understood, is the  expressionof an opinion after due consideration of  the  facts  which  deserve  to  bedetermined. Without pronouncement of a judgment in the  open  court,  signedand dated, it is difficult to treat it as a judgment of  conviction  as  hasbeen held in Re. Athipalayan and Ors[7].   As a matter of fact, on  inquiry,the High Court in the administrative side had found there  was  no  judgmentavailable on record.  Learned counsel for the appellants would  submit  thatin the counter affidavit filed by the High Court it has been mentioned  thatan incomplete typed  judgment  of  14  pages   till  paragraph  No.  19  wasavailable.  The affidavit also states that it was  incomplete  and  no  pagehad the signature  of  the  presiding  officer.   If  the  judgment  is  notcomplete and signed, it cannot be a judgment in terms of Section  353  CrPC.It is unimaginable that a judgment  is  pronounced  without  there  being  ajudgment.  It is gross illegality. In  this  context,  we  may  refer  to  apassage from State  of  Punjab   and  others  v.  Jagdev  Singh  Talwandi[8]wherein expressing the opinion  for  the  Constitution  Bench,  Chandrachud,C.J. observed thus:-\n",
            "to be added {'tagged_entities': [[1502, 1513, 'Section  353', 'Section'], [1516, 1522, 'CrPC.It', 'Act']]}\n",
            "else\n",
            "changed para_id 47\n",
            "content_id p_47\n",
            "___________________________\n",
            "para_id to send 47\n",
            "all_together  30. We would like to take  this  opportunity  to  point  out  that  seriousdifficulties arise on account of the practice increasingly  adopted  by  theHigh Courts, of pronouncing the final order without a reasoned judgment.  Itis desirable that the final order which  the  High  Court  intends  to  passshould  not  be  announced  until  a  reasoned   judgment   is   ready   forpronouncement. Suppose, for example, that a final order without  a  reasonedjudgment is announced by the High Court that a house  shall  be  demolished,or that the custody of a child  shall  be  handed  over  to  one  parent  asagainst the other,  or  that  a  person  accused  of  a  serious  charge  isacquitted, or that a statute is  unconstitutional  or,  as  in  the  instantcase, that a detenu be released from detention. If  the  object  of  passingsuch orders is to ensure speedy compliance with them, that  object  is  moreoften defeated by the aggrieved party filing a  special  leave  petition  inthis Court against the order passed by the  High  Court.  That  places  thisCourt in a predicament because, without the benefit of the reasoning of  theHigh Court, it is difficult for this Court to allow the  bare  order  to  beimplemented. The result inevitably  is  that  the  operation  of  the  orderpassed by the High Court has to be stayed pending delivery of  the  reasonedjudgment.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 48\n",
            "content_id p_48\n",
            "___________________________\n",
            "para_id to send 48\n",
            "all_together  31. It may be thought  that  such  orders  are  passed  by  this  Court  andtherefore there is no reason why the High Courts should not do the same.  Wewould like to point out respectfully that the orders passed  by  this  Courtare final and no appeal lies against them. The Supreme Court  is  the  finalcourt in the hierarchy of our courts. Besides,  orders  without  a  reasonedjudgment  are  passed  by  this  Court  very   rarely,   under   exceptionalcircumstances. Orders passed by the High Court are subject to the  appellatejurisdiction of this Court under Article 136 of the Constitution  and  otherprovisions of the concerned statutes. We thought it necessary to make  theseobservations in order that a practice which is not very desirable and  whichachieves no useful purpose may not grow out of its present infancy.\n",
            "to be added {'tagged_entities': [[566, 576, 'Article 136', 'Article'], [585, 596, 'Constitution', 'Act']]}\n",
            "else\n",
            "changed para_id 49\n",
            "content_id p_49\n",
            "___________________________\n",
            "para_id to send 49\n",
            "all_together  18.   We have reproduced the aforesaid two passages as the larger Bench  hasmade such  observations  with  regard  to              unreasoned  judgmentspassed by the High Courts. The learned Chief  Justice  had  noted  that  thepractice is not desirable and does not achieve any  useful  purpose  and  itshould not grow out of its present infancy.  Despite the said  observations,sometimes this Court comes  across  judgments  and  orders  where  the  HighCourts have announced  the  result  of  the  case  by  stating  reasons  tofollow. We can only reiterate the observations of the Constitution Bench.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 50\n",
            "content_id p_50\n",
            "___________________________\n",
            "para_id to send 50\n",
            "all_together  19.    Having stated  that,  as  is  evincible  in  the  instant  case,  thejudgment is not available on record and hence, there can  be  no  shadow  ofdoubt that the declaration of the result cannot tantamount to a judgment  asprescribed in the CrPC.  That leads to the inevitable  conclusion  that  thetrial in both the cases has to be treated to be pending.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 51\n",
            "content_id p_51\n",
            "___________________________\n",
            "para_id to send 51\n",
            "all_together  20.   The next issue that emerges for  consideration  is  whether  the  HighCourt on its administrative side could have transferred the  case  from  theSecond Additional Sessions Judge, Ambikapur to the  Court  of  District  andSessions Judge, Surguja at Ambikapur. In  this  regard,  it  is  suffice  tounderstand the jurisdiction and authority conferred under  the  Constitutionon the High Court in the prescription  of  power  of  superintendence  underArticle 227. Article 227 of the Constitution reads as follows:-227. Power of superintendence over all courts by the High  Court:-(1)EveryHigh  Court  shall  have  superintendence  over  all  courts  and  tribunalsthroughout the territories in relation to which it exercises jurisdiction.(2)Without prejudice to the generality of  the  foregoing  provisions,  theHigh Court may-\n",
            "to be added {'tagged_entities': [[470, 476, 'Article', 'Article'], [478, 480, '227', 'Section'], [489, 500, 'Constitution', 'Act']]}\n",
            "else\n",
            "changed para_id 52\n",
            "content_id p_52\n",
            "___________________________\n",
            "para_id to send 52\n",
            "all_together  (a)call for returns from such courts;\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 53\n",
            "content_id p_53\n",
            "___________________________\n",
            "para_id to send 53\n",
            "all_together  (b)make and issue general rules and  prescribe  forms  for  regulating  thepractice and proceedings of such courts; and\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 54\n",
            "content_id p_54\n",
            "___________________________\n",
            "para_id to send 54\n",
            "all_together  (c)prescribe forms in which books, entries and accounts shall  be  kept  bythe officers of any such courts(3)The High Court may also settle tables of  fees  to  be  allowed  to  thesheriff and all clerks  and  officers  of  such  courts  and  to  attorneys,advocates and pleaders practising therein:\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 55\n",
            "content_id p_55\n",
            "___________________________\n",
            "para_id to send 55\n",
            "all_together  Provided that any rules made, forms prescribed or tables settled  underclause (2) or clause (3) shall not be inconsistent  with  the  provision  ofany law for the  time  being  in  force,  and  shall  require  the  previousapproval of the Governor(4)Nothing in this article shall be  deemed  to  confer  on  a  High  Courtpowers of superintendence over any  court  or  tribunal  constituted  by  orunder any law relating to the Armed Forces.      The aforesaid Article confers power of  superintendence  on  the  HighCourt over the courts and tribunals within the territory of the  State.  TheHigh Court has the jurisdiction and  the  authority  to  exercise  suo  motupower.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 56\n",
            "content_id p_56\n",
            "___________________________\n",
            "para_id to send 56\n",
            "all_together  21.   In Achutananda Baidya v. Prafullya Kumar Gayen and  others[9]  a  two-Judge Bench while dealing with the power  of  superintendence  of  the  HighCourt under Article 227 has opined that the power of superintendence of  theHigh Court under  Article  227  of  the  Constitution  is  not  confined  toadministrative superintendence only  but  such  power  includes  within  itssweep the power of judicial review. The power and duty  of  the  High  Courtunder Article 227 is essentially to ensure that the  courts  and  tribunals,inferior to High Court, have done what they were  required  to  do.  Law  iswell settled by various decisions of this Court  that  the  High  Court  caninterfere under Article 227  of  the  Constitution  in  cases  of  erroneousassumption  or  acting  beyond  its  jurisdiction,   refusal   to   exercisejurisdiction, error of law apparent on record as distinguished from  a  meremistake  of  law,  arbitrary  or  capricious  exercise   of   authority   ordiscretion, a patent error in procedure, arriving  at  a  finding  which  isperverse or based on no material, or resulting in manifest injustice.\n",
            "to be added {'tagged_entities': [[165, 175, 'Article 227', 'Article'], [247, 258, 'Article  227', 'Article'], [270, 281, 'Constitution', 'Act'], [463, 473, 'Article 227', 'Article'], [701, 711, 'Article 227', 'Article'], [723, 734, 'Constitution', 'Act']]}\n",
            "else\n",
            "changed para_id 57\n",
            "content_id p_57\n",
            "___________________________\n",
            "para_id to send 57\n",
            "all_together  22.   We have already stated that the Division Bench while  concurring  withthe opinion of the learned single Judge has also quashed the  order  by  thelearned trial judge on the ground that there  was  no  judgment  on  record.There is no dispute about the fact that the Full Court  of  the  High  Courtafter coming to a definite conclusion  that  the  learned  trial  judge  hadreally not passed any judgment, resolved that the matter should be heard  bythe learned Sessions Judge and accordingly  the  Registrar  General  of  theHigh Court communicated the  decision  to  the  concerned  learned  SessionsJudge. The submission of the learned counsel for the appellant is that  sucha  power  could  not  have  been  exercised  by  the  Full  Court   on   theadministrative side, for in exercise of administrative authority,  the  HighCourt cannot transfer the case. The contention is that High Court  can  onlytransfer the case in exercise of power under Section 407  and  that  too  onthe judicial side. Our attention has also been  drawn  to   Section  194  ofCrPC. Section 194 empowers the Additional and Assistant Sessions  Judges  totry cases made over to them. The said provision reads as follows:-194. Additional and Assistant Sessions Judges to try  cases  made  over  tothem.? An Additional Sessions Judge or Assistant Sessions  Judge  shall  trysuch cases as the Sessions Judge of the division may, by general or  specialorder, make over to him for trial or as  the  High  Court  may,  by  specialorder, direct him to try.\n",
            "to be added {'tagged_entities': [[958, 968, 'Section 407', 'Section'], [1049, 1060, 'Section  194', 'Section'], [1071, 1081, 'Section 194', 'Section']]}\n",
            "else\n",
            "changed para_id 58\n",
            "content_id p_58\n",
            "___________________________\n",
            "para_id to send 58\n",
            "all_together  23.   It is argued that Section 194 can be exercised on  the  administrativeside before the commencement  of  the  trial  and  not  thereafter,  whereasSection 407 can be taken recourse to on the judicial side and a case can  betransferred on the basis of parameters laid down for transfer of a  criminaltrial. In this regard, we may usefully refer  to  the  authority  in  RanbirYadav v. State of Bihar[10] wherein under  certain  circumstances  the  HighCourt had transferred the sessions trial from the court  of  one  AdditionalSessions Judge to another by an administrative order at  a  stage  when  thetrial had commenced.  It was contended before  this  Court  that  the  trialthat took place before the transferee court was wholly without  jurisdictionand consequently the conviction and sentence recorded  by  that  court  werenull and void and were not curable under Section 465 CrPC.  To  sustain  thesaid proposition of law, reliance was placed in A.R. Antulay v.  R.S.  Nayakand another[11].  The two-Judge Bench perusing the material on  record  cameto the conclusion that the order  was  passed  by  the  High  Court  in  itsadministrative jurisdiction. Thereafter, it proceeded to opine thus:-Under Article 227 of  the  Constitution  of  India  every  High  Court  hassuperintendence over all courts and tribunals throughout the territories  inrelation to which it exercises jurisdiction and it is trite that this  powerof  superintendence  entitles  the   High   Court   to   pass   orders   foradministrative exigency and expediency. In the instant case it appears  thatthe High Court had exercised the power of transfer in  the  context  of  thepetition filed by some of the accused from jail complaining that they  couldnot be accommodated in the courtroom as a result of which some of  them  hadto remain outside. It further appears that the other  grievance  raised  wasthat the court was so crowded that even  clerks  of  the  lawyers  were  notbeing allowed to enter the courtroom to carry the briefs. Such  a  situationwas obviously created by the trial of a large number of persons. If  in  thecontext  of  the  above  facts,  the  High  Court  exercised   its   plenaryadministrative power to transfer the  case  to  the  5th  Court,  which,  weassume had a bigger and  better  arrangement  to  accommodate  the  accused,lawyers and others connected with the trial no exception  can  be  taken  tothe same, particularly by those at whose instance and for whose benefit  thepower was exercised.Proceeding further, the Court held that:-\n",
            "to be added {'tagged_entities': [[25, 35, 'Section 194', 'Section'], [878, 888, 'Section 465', 'Section'], [890, 893, 'CrPC', 'Act'], [974, 974, 'v', 'Case'], [1217, 1227, 'Article 227', 'Article'], [1238, 1249, 'Constitution', 'Act']]}\n",
            "else\n",
            "changed para_id 59\n",
            "content_id p_59\n",
            "___________________________\n",
            "para_id to send 59\n",
            "all_together  So long as  power  can  be  and  is  exercised  purely  for  administrativeexigency without impinging upon and prejudicially affecting  the  rights  orinterests of the parties to any judicial  proceeding  we  do  not  find  anyreason to hold that administrative  powers  must  yield  place  to  judicialpowers  simply  because  in  a  given  circumstance  they  coexist.  On  thecontrary, the  present  case  illustrates  how  exercise  of  administrativepowers were more expedient, effective and efficacious.  If  the  High  Courthad intended to exercise its judicial powers of  transfer  invoking  Section407 of  the  Code  it  would  have  necessitated  compliance  with  all  theprocedural formalities thereof, besides providing adequate opportunities  tothe parties of a proper hearing which,  resultantly,  would  have  not  onlydelayed the trial but further incarceration of some of the  accused.  It  isobvious, therefore, that by invoking its power of  superintendence,  insteadof judicial powers, the High Court not only redressed the grievances of  theaccused and  others  connected  with  the  trial  but  did  it  with  utmostdispatch.\n",
            "to be added {'tagged_entities': [[602, 611, 'Section407', 'Section']]}\n",
            "else\n",
            "changed para_id 60\n",
            "content_id p_60\n",
            "___________________________\n",
            "para_id to send 60\n",
            "all_together  24.   The Court distinguished the authority in A.R. Antulay case (supra)  onthe basis that in the said case the  Court  was  dealing  with  a  situationwhere this Court had transferred the case to the High Court  which  was  notauthorized by law and the Court could not have conferred  the  jurisdictionson the High Court as it did not possess such jurisdiction under  the  schemeof the Criminal Law Amendment Act,  1952.   The  controversy  the  two-JudgeBench was dealing with pertained to transfer of  the  case  to  the  learnedAdditional Sessions Judge who was competent under the CrPC  to  conduct  thesessions trial and, therefore, the Court  in  Ranbir  Yadavs  case  (supra)ruled that the order of transfer to another court did not  suffer  from  anylegal infirmity.\n",
            "to be added {'tagged_entities': [[411, 420, 'Act,  1952', 'Act'], [587, 590, 'CrPC', 'Act']]}\n",
            "else\n",
            "changed para_id 61\n",
            "content_id p_61\n",
            "___________________________\n",
            "para_id to send 61\n",
            "all_together  25.   In the case at hand, the High Court on  the  administrative  side  hadtransferred the  case  to  the  learned  Sessions  Judge  by  which  it  hasconferred jurisdiction on the trial court which has the jurisdiction to  trythe sessions case under CrPC.  Thus, it has done so as it has, as  a  matterof  fact,  found  that  there  was  no  judgment  on  record.  There  is  noillegality.  Be it noted, the Division Bench in the appeal preferred at  theinstance of the present appellants  thought  it  appropriate  to  quash  theorder as there is no judgment  on  record  but  a  mere  order-sheet.  In  apiquant situation like the present one, we are disposed to  think  that  theHigh Court was under legal obligation to set aside the order as  it  had  noeffect in law.  The High Court has correctly done so as it has the  duty  tosee that sanctity of justice is not undermined.   The High  Court  has  doneso as it has felt that an order  which  is  a  mere  declaration  of  resultwithout the judgment should be nullified and become extinct.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 62\n",
            "content_id p_62\n",
            "___________________________\n",
            "para_id to send 62\n",
            "all_together  26.   The case at hand constrains us  to  say  that  a  trial  Judge  shouldremember that he has immense responsibility as  he  has  a  lawful  duty  torecord the evidence in the prescribed manner keeping  in  mind  the  commandpostulated  in              Section  309  of  the  CrPC  and  pronounce  thejudgment as provided under the Code. A Judge in charge of the trial  has  tobe extremely diligent so that no dent is created in the  trial  and  in  itseventual conclusion. Mistakes made or errors committed are to  be  rectifiedby the appellate court in exercise  of  error  jurisdiction.   That  is  adifferent matter. But, when a situation like the present one  crops  up,  itcauses agony, an unbearable one, to the cause of justice  and  hits  like  alightning in a cloudless sky.  It hurts the justice dispensation system  andno one, and we mean no one, has any right  to  do  so.  The  High  Court  byrectifying the grave  error  has  acted  in  furtherance  of  the  cause  ofjustice.  The accused persons might have felt  delighted  in  acquittal  andaffected by the order of rehearing, but they should bear in mind  that  theyare not the lone receivers of justice.  There  are  victims  of  the  crime.Law serves both and justice looks at them equally.   It  does  not  toleratethat the grievance of the victim should be comatosed in this manner.\n",
            "to be added {'tagged_entities': [[257, 268, 'Section  309', 'Section'], [280, 283, 'CrPC', 'Act']]}\n",
            "else\n",
            "changed para_id 63\n",
            "content_id p_63\n",
            "___________________________\n",
            "para_id to send 63\n",
            "all_together  27.   Consequently, appeals are dismissed.  The  trial  court  to  whom  thecases have been transferred is directed to proceed in accordance with law.\n",
            "to be added {'tagged_entities': []}\n",
            "else\n",
            "changed para_id 64\n",
            "content_id p_64\n",
            "___________________________\n",
            "para_id 64\n",
            "content_id subpara_1\n",
            "checking for last True\n",
            "para_id to send 64\n",
            "all_together  .............................J.[DipakMisra]New Delhi;                              .............................J.January  06, 2017                               [Amitava Roy]-----------------------[1]    (2002) 1 SCC 319[2]    (2002) 8 SCC 400[3]    (2003) 6 SCC 675[4]    (2010) 2 SCC 398[5]    (1981) 1 SCC 500[6]    AIR 1962 SC 1208[7]    AIR 1960 Mad 507[8]    (1984) 1 SCC 596[9]    (1997) 5 SCC 76[10]   (1995) 4 SCC 392[11]   (1988) 2 SCC 602\n",
            "to be added {'tagged_entities': []}\n",
            "success!\n",
            "<_io.TextIOWrapper name='updated_json.json' mode='w' encoding='UTF-8'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC9pDBqpgeIg"
      },
      "source": [
        "    #### not needed ####\n",
        "    # if input_csv['ParaID'].iloc[-1] == para_id: \n",
        "    #   last_counter += 1\n",
        "      # prev_para_id = para_id\n",
        "      # if last_counter == len(paragraphs)-1:\n",
        "    #   print(\"para_id to send\", para_id)\n",
        "    #   df = df.append(modelPredict(all_together))\n",
        "    #   print(\"all_together\",all_together)\n",
        "    #   updated_data = appendToJson(df,json_data,para_id, all_together)  # calling appendToJson # changed\n",
        "    #   all_together = \" \"\n",
        "    #   break\n",
        "      # continue"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}