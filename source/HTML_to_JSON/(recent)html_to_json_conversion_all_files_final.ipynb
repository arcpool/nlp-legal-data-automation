{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9021b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff84946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b30955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"extracting cites (including edge cases for \"view all\" and None)\"\"\"\n",
    "\n",
    "def find_cites(soup):\n",
    "    \n",
    "    baseurl = 'https://indiankanoon.org'\n",
    "    cites_list = []  # final cites list \n",
    "    main_cites = soup.find('div',class_ = 'doc_cite')  # extracting all document citations(cites and citedby)\n",
    "    if main_cites != None:\n",
    "        view_all_link = main_cites.a['href']  # to get the link to extract more 'cites' - view all\n",
    "        if \":\" in view_all_link:\n",
    "            page_num = 0\n",
    "            while True:\n",
    "                tempurl = \"https://indiankanoon.org/search/?formInput=cites%3A%20\"+ view_all_link.split(\":\")[1]+\"&pagenum=\" \n",
    "                tempurl += str(page_num)\n",
    "                while True:\n",
    "                    try:\n",
    "                        r = requests.get(tempurl)\n",
    "                        break\n",
    "                    except(AttributeError, requests.exceptions.MissingSchema, requests.exceptions.ConnectionError, requests.exceptions.InvalidURL,requests.exceptions.InvalidSchema):\n",
    "                        print(\"Request Error\")\n",
    "                soup_again = BeautifulSoup(r.content, 'html')\n",
    "                all_cites_link = soup_again.select(\".result_title\")\n",
    "                for link in all_cites_link :\n",
    "                    cites_list.append({\"title\":link.find('a').text,\"link\": baseurl + link.find('a').get('href'),\"caseids\":[\"0\"]})\n",
    "\n",
    "                if len(all_cites_link) != 10:  # to break the infinite loop\n",
    "                    break\n",
    "                page_num += 1\n",
    "\n",
    "        else:\n",
    "            only_cite_heads = soup.select(\"div > .doc_cite_head\")  # array of cites and citedby headers\n",
    "            if \"Cites\" in only_cite_heads[0].text:  # checking if cites are present in the html\n",
    "                res = [int(i) for i in only_cite_heads[0].text.split() if i.isdigit()]  # getting the number of cites to extract\n",
    "                all_cites_link = soup.select(\"div > .cite_title\")[:res[0]]\n",
    "                for link in all_cites_link:\n",
    "                    cites_list.append({\"title\":link.find('a').text,\"link\":baseurl + link.find('a').get('href'),\"caseids\":[\"0\"]})\n",
    "\n",
    "    return cites_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b35f1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"extracting Cited By (including edge cases for \"view all\" and None)\"\"\"\n",
    "\n",
    "def find_cited_by(soup):\n",
    "    \n",
    "    baseurl = 'https://indiankanoon.org'\n",
    "    cited_by_header = soup.select('a[href*=\"citedby\"]')  # extracting the header and its content which contains \"citedby\"\n",
    "    cited_by_list = []  # final Cited By list\n",
    "    if cited_by_header != []:  # Checking if there are no Cited By in the HTML\n",
    "        view_all_link = cited_by_header[0].parent.a['href']  # to get the link to extract more 'cited by' - view all\n",
    "        if \":\" in view_all_link:\n",
    "            page_num = 0\n",
    "            while True:\n",
    "                tempurl = \"https://indiankanoon.org/search/?formInput=citedby%3A%20\"+ view_all_link.split(\":\")[1]+\"&pagenum=\" \n",
    "                tempurl += str(page_num)\n",
    "                while True:\n",
    "                    try:\n",
    "                        r = requests.get(tempurl)\n",
    "                        break\n",
    "                    except(AttributeError, requests.exceptions.MissingSchema, requests.exceptions.ConnectionError, requests.exceptions.InvalidURL,requests.exceptions.InvalidSchema):\n",
    "                        print(\"Request Error\")\n",
    "                soup_again = BeautifulSoup(r.content, 'html')\n",
    "                all_cited_by_links = soup_again.select(\".result_title\")\n",
    "                \n",
    "                for link in all_cited_by_links:\n",
    "                    cited_by_list.append({\"title\":link.find('a').text,\"link\": baseurl + link.find('a').get('href'),\"caseids\":[\"0\"]})\n",
    "                    \n",
    "                if len(all_cited_by_links) != 10:  # to break the infinite loop\n",
    "                    break\n",
    "                page_num += 1\n",
    "                \n",
    "    else:\n",
    "        main_cites = soup.find('div',class_ = 'doc_cite')  # extracting all document citations(cites and citedby)\n",
    "        only_cite_heads = soup.select(\"div > .doc_cite_head\")  # array of cites and citedby headers\n",
    "        if only_cite_heads != []:\n",
    "            if len(only_cite_heads) > 1:  # if only_cite_heads[1] contains cited by links\n",
    "                all_cited_by_links = only_cite_heads[1].find_next_siblings(\"div\")  # iteratively find all links in the same tree\n",
    "                for link in all_cited_by_links:\n",
    "                    cited_by_list.append({\"title\": link.find('a').text,\"link\":baseurl + link.find('a').get('href'),\"caseids\":[\"0\"]})\n",
    "\n",
    "            elif \"Citedby\" in only_cite_heads[0].text: # if there are only cited by links present in the HTML (no cites)\n",
    "                all_cited_by_links = only_cite_heads[0].find_next_siblings(\"div\") #find siblings\n",
    "                for link in all_cited_by_links:\n",
    "                    cited_by_list.append({\"title\": link.find('a').text,\"link\":baseurl + link.find('a').get('href'),\"caseids\":[\"0\"]})\n",
    "    \n",
    "    return cited_by_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23380e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating basic caseinfo dictionary of cites and citedby\"\"\"\n",
    "\n",
    "def create_caseinfo(soup):\n",
    "    \n",
    "    cites = find_cites(soup)\n",
    "    cited_by = find_cited_by(soup)\n",
    "    court = soup.find('div',class_ = 'docsource_main').text\n",
    "    caseinfo = {'court': court.strip(), 'cites': cites,'citedBy': cited_by}\n",
    "    return caseinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a076494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code adapted from data acquisition pipeline code to extract data from HTML\"\"\"\n",
    "\n",
    "def sortdata(soup, caseinfo, alltext):\n",
    "    \n",
    "    initial = alltext.find('pre', id = 'pre_1')\n",
    "    vslist = ['versus', 'vs', 'vs.', 'Versus']\n",
    "    petitioners = []\n",
    "    respondents = ''\n",
    "    flag = 0\n",
    "    baseurl = 'https://indiankanoon.org'\n",
    "\n",
    "    # Application name and number\n",
    "    test_list = []\n",
    "    if initial:\n",
    "        test_list = [i.strip() for i in initial.text.splitlines() if i]\n",
    "    applicationname = '0'\n",
    "    applicationnumber = '0'\n",
    "    for test in test_list:\n",
    "        if test.lower() == 'in the supreme court of india' or test.lower() == 'in the high court of judicature of bombay' or test.lower() == 'case no.:' or test.lower().find('in the high court') != -1:\n",
    "            applicationname = '1'\n",
    "        elif applicationname == '1':\n",
    "            applicationname = test\n",
    "            applicationnumber = '1'\n",
    "        elif applicationnumber == '1':\n",
    "            applicationnumber = test\n",
    "    if alltext.find('p', id='p_1') and applicationname == \"0\" and applicationnumber == \"0\":\n",
    "        temp = alltext.find('p', id='p_1').text.strip().split(' ')\n",
    "        temp2 = alltext.find('p', id='p_1').text.split(':')\n",
    "        try:\n",
    "            if temp[1].strip().lower() == 'appellate' or temp2[0].strip().lower() == 'writ' and temp2[1].strip().lower() == 'petition':\n",
    "                ans = alltext.find('p', id='p_1').text.split(':')\n",
    "                # print(ans)\n",
    "                applicationname = ans[0].strip()\n",
    "                if len(ans) != 1:\n",
    "                    applicationnumber = ans[1].strip()\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "    caseinfo['applicationName'] = applicationname\n",
    "    caseinfo['applicationNumber'] = applicationnumber\n",
    "\n",
    "    author = '0'\n",
    "    if alltext.find('div', class_ = 'doc_author'):\n",
    "        author = alltext.find('div', class_ = 'doc_author').text.split(':')[1].strip()\n",
    "    caseinfo['author'] = author\n",
    "\n",
    "    # Bench\n",
    "    bench = '0'\n",
    "    if alltext.find('div', class_='doc_bench'):\n",
    "        bench = alltext.find('div', class_='doc_bench').text.split(':')[1].strip()\n",
    "        bench = [{\"name\":a_bench, \"id\":\"0\"} for a_bench in bench.split(\",\")]\n",
    "\n",
    "    caseinfo['bench'] = bench\n",
    "\n",
    "    link = baseurl + str(soup.find('form').get(\"action\").split(\"nextpage=\")[-1])  # changed link\n",
    "    caselink = link\n",
    "    casename = soup.find('title').text.strip().replace('/', '').replace('*', '').replace('\\\\', '').replace('<', '').replace('>', '').replace(':', '').replace('?', '').replace('\"', '').replace('|', '').replace('\\t', '')\n",
    "\n",
    "    paras = alltext.find_all(['p', 'blockquote','pre'])[1:]  # 'blockquote'\n",
    "\n",
    "    if not initial:\n",
    "\n",
    "        if alltext.find('p', id = 'p_1') and alltext.find('p', id = 'p_1').text.split(' ')[0].strip() in ['JUDGMENT', 'J U D G M E N T', 'ORDER']:\n",
    "            judge = (alltext.find('p', id='p_1').text.replace('JUDGMENT', '').replace('J U D G M E N T', '').replace('ORDER', '').strip())\n",
    "\n",
    "        else:\n",
    "            judge = '0'\n",
    "\n",
    "    else:\n",
    "        judgement = (initial.text.splitlines()[-1].strip().replace(' ',''))\n",
    "        if alltext.find('p', id='p_1') != None:\n",
    "            if len(alltext.find('p', id='p_1').text) < 25 and alltext.find('p', id='p_1') and (judgement.lower().replace(':', '') == 'judgment' or alltext.find('p', id = 'p_1').text.lower().find('judgment') != -1 or alltext.find('p', id = 'p_1').text.lower().find('j u d g m e n t ') != -1):\n",
    "                judge = (alltext.find('p', id = 'p_1').text.replace('JUDGMENT', '').replace('J U D G M E N T', '').strip())\n",
    "                paras = paras[1:]\n",
    "            else:\n",
    "                judge = (judgement)\n",
    "        else:\n",
    "            judge = \"\"\n",
    "\n",
    "    if judge == \"\":\n",
    "        judge = '0'\n",
    "    if judge.lower().find('order') != -1 or judge.lower().find('respondent') != -1:\n",
    "        judge = '0'\n",
    "\n",
    "    judge = [{\"name\":judge, \"id\":\"0\"}]\n",
    "\n",
    "    caseinfo['judge'] = judge\n",
    "\n",
    "    casetitle = soup.find('title').text.strip().replace('/', '').replace('*', '').replace('\\\\', '').replace('<', '').replace('>', '').replace(':', '').replace('?', '').replace('\"', '').replace('|', '').replace('\\t', '')\n",
    "\n",
    "    # Petitioners and Respondents\n",
    "    if initial:\n",
    "        text = ''\n",
    "        vsflag = 0\n",
    "        for word in initial.text.splitlines():\n",
    "            word = (word.strip())\n",
    "            if not word:\n",
    "                text = ''\n",
    "                continue\n",
    "            else:\n",
    "                text = text + ' ' + word\n",
    "\n",
    "            if flag == 2:\n",
    "                respondents += ' ' + word\n",
    "\n",
    "            if flag == 1:\n",
    "                respondents = text\n",
    "                flag = 2\n",
    "\n",
    "            if word.replace('-', '').replace('/', '').replace('.', '').lower().strip() in vslist:\n",
    "                vsflag = 1\n",
    "                petitioners.append(prev)\n",
    "                text = ''\n",
    "                flag = 1\n",
    "            prev = text\n",
    "        petitioners = [item.split('   ')[0].replace('PETITIONER:', '').strip() for item in petitioners]\n",
    "\n",
    "        if vsflag == 0:\n",
    "            pet = 0\n",
    "            res = 0\n",
    "            for word in initial.text.splitlines():\n",
    "                if word.strip().replace(':', '').replace('-', '').lower() == 'petitioner':\n",
    "                    pet = 1\n",
    "                elif word.strip().replace(':', '').replace('-', '').lower() == 'respondent':\n",
    "                    res = 1\n",
    "                elif pet == 1:\n",
    "                    petitioners.append(word.strip())\n",
    "                    pet = 0\n",
    "                elif res == 1:\n",
    "                    respondents = word.strip()\n",
    "                    res = 0\n",
    "\n",
    "    respondents = respondents.replace('RESPONDENT:', '').split('   ')[0].strip()\n",
    "\n",
    "    if len(petitioners) == 0 or respondents == \"\" or len(petitioners[0]) < 5 or len(respondents) < 5:\n",
    "        temp = casetitle.split('vs')\n",
    "        petitioners = []\n",
    "        petitioners.append(temp[0].strip())\n",
    "        if len(temp)>1:\n",
    "            respondents = temp[1].split(' on ')[0].strip()\n",
    "\n",
    "    petitioners = list(dict.fromkeys(petitioners))\n",
    "\n",
    "    # Paragraphs\n",
    "    allparas = []\n",
    "    mainparas = {}\n",
    "    subparas = {}\n",
    "    blockq = []    \n",
    "    for  i in range(1,len(paras)) :\n",
    "        if paras[i].name == \"p\":\n",
    "            mainparas=[paras[i].text.strip().replace('\\n', ' ').replace('\\t', ' ').replace('\\x0c', '')]\n",
    "\n",
    "            subparas={}\n",
    "            blockq = []        \n",
    "            if i+1 != len(paras):                               \n",
    "                if paras[i+1].name == \"pre\":\n",
    "                    sub = {}  \n",
    "                    q =i+1\n",
    "                    while (paras[q].name !=\"p\"):\n",
    "                        sub[paras[q].get('id')] = paras[q].text.strip().replace('\\n', ' ').replace('\\t', ' ').replace('\\x0c', '')\n",
    "                        q=q+1\n",
    "\n",
    "                        if q ==len(paras):\n",
    "                            break\n",
    "                    subparas =sub\n",
    "                if paras[i+1].name ==\"blockquote\":\n",
    "                    block_quote = []\n",
    "                    q= i+1\n",
    "                    while (paras[q].name !=\"p\"):\n",
    "                        block_quote.append(paras[q].text.strip().replace('\\n', ' ').replace('\\t', ' ').replace('\\x0c', ''))\n",
    "                        q = q+1\n",
    "\n",
    "                        if q == len(paras):\n",
    "                            break\n",
    "\n",
    "                    blockq = [\"\\n \".join(block_quote)] \n",
    "            else:\n",
    "                pass\n",
    "            allparas.append({'para': mainparas,  'blockQuotes': blockq,'subPara': subparas,})\n",
    "\n",
    "    # Combining all together\n",
    "    caseinfo['title'] = casetitle\n",
    "    caseinfo['petitioners'] = petitioners\n",
    "    caseinfo['respondants'] = respondents\n",
    "    caseinfo['link'] = caselink\n",
    "\n",
    "    prejudgement = \"\"\n",
    "    if initial != None:\n",
    "        prejudgement = initial.text\n",
    "\n",
    "    equivalent_citations = []    \n",
    "    caseinfo[\"prejudgement\"] = [prejudgement]\n",
    "    caseinfo['paragraphs'] = allparas\n",
    "    caseinfo['equivalentCitations'] = equivalent_citations\n",
    "    caseinfo['id'] = \"0\"\n",
    "\n",
    "    return caseinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4bf103",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Successfully converted all HTML(s) to JSON(s)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main method to change HTML path according to the user\"\"\"\n",
    "\n",
    "def main():\n",
    "    \n",
    "    data_folder_path = r'/Users/arya/anydesk_transfer/J&K'  # HTML files path\n",
    "    duplicate_index_counter = 0\n",
    "    for court in os.listdir(data_folder_path):\n",
    "        if not court.endswith(\".zip\"):\n",
    "            if court == \"Jammu & Kashmir\":\n",
    "                for year in sorted(os.listdir(data_folder_path+\"/\"+court)):\n",
    "                    if year == \".DS_Store\":\n",
    "                        continue\n",
    "                    if int(year) >= int(\"1800\"):\n",
    "                        for month in sorted(os.listdir(data_folder_path+\"/\"+court+\"/\"+year)):\n",
    "                            if month == \".DS_Store\":\n",
    "                                continue\n",
    "                            for a_html in Path(data_folder_path+\"/\"+court+\"/\"+year+\"/\"+month).rglob('*.html'):\n",
    "                                soup = BeautifulSoup(open(a_html,'r', encoding='utf-8'),'html.parser')\n",
    "                                case_name = str(a_html).split(\"\\\\\")[-1]\n",
    "                                alltext = soup.find('div', class_ = 'judgments')\n",
    "                                caseinfo_raw = create_caseinfo(soup)\n",
    "                                caseinfo = sortdata(soup, caseinfo_raw, alltext)\n",
    "                                save = os.path.join(Path(data_folder_path+\"/\"+court+\"/\"+year+\"/\"+month), case_name.replace(\".html\",\".json\"))\n",
    "                                with open(save, \"w\") as outfile:\n",
    "                                    json.dump(caseinfo, outfile, indent = 4)\n",
    "                                outfile.close()\n",
    "                                        \n",
    "    print(\"Successfully converted all HTML(s) to JSON(s)\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78371a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca2d8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "interpreter": {
   "hash": "4f79772460874e398d5bd1bab2d8d24d63a71d9bd1f291b74745bf8f730f9e07"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}