{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import json\n",
    "from pprint import pprint\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "import nltk\n",
    "import functools\n",
    "import tokenizations\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import os.path\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import sentencepiece as spm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractJson(json_file_path,converted_txt_path): # reading json\n",
    "    with open(json_file_path, 'r') as jsonfile:\n",
    "        json_data = json.loads(jsonfile.read())\n",
    "        paragraphs = json_data['paragraphs']\n",
    "\n",
    "    converted_txt = open(converted_txt_path,'w')\n",
    "\n",
    "    for para in paragraphs:\n",
    "        mainPara = \"\"\n",
    "        for item in para['para']:\n",
    "            mainPara += item\n",
    "        converted_txt.write(str(mainPara))\n",
    "            \n",
    "        converted_txt.write('\\n')\n",
    "        subPara = para['subPara']\n",
    "        \n",
    "        for k,v in subPara.items():\n",
    "            converted_txt.write(str(v))\n",
    "            \n",
    "        converted_txt.write('\\n')\n",
    "        blockQ = \"\"\n",
    "        \n",
    "        for i in para['blockQuotes']:\n",
    "            blockQ += i\n",
    "            \n",
    "        converted_txt.write(str(blockQ))\n",
    "\n",
    "        converted_txt.write('\\n')\n",
    "    converted_txt.close()\n",
    "    jsonfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenize(document): \n",
    "    \n",
    "    model = io.BytesIO()\n",
    "\n",
    "    sp = spm.SentencePieceProcessor(model_proto=model.getvalue())\n",
    "\n",
    "    trainer = PunktTrainer()\n",
    "\n",
    "    mangle = functools.partial(re.sub, r'([MD]rs?[.]) ([A-Z])', r'\\1_\\2')\n",
    "\n",
    "    sent_tokenize = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#     trainer.train(document)\n",
    "    sample = mangle(document)\n",
    "\n",
    "    tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "    excp_list = [\"anr\", \"supp\",\"art\",\"p\",\"no\",\"v\",\"ltd\",\"supra\",\"vs\",\"co\",\"mr\",\"dr\",\"ex\",\"s\",\"sec\",\"section\",\"sections\",\"act\",\"art\",\"article\",\"eg\",\"mohd\",\"etc\",\"suppl\",\"vs\",\"(\",\"[\",\"rs\",\"asst\",\"smt\"]\n",
    "    tokenizer._params.abbrev_types.update(excp_list)\n",
    "    tokenize_sentence = []\n",
    "        \n",
    "    for a_sent in tokenizer.tokenize(document):\n",
    "        string_split = word_tokenize(a_sent)\n",
    "        word_count = len(string_split)\n",
    "        \n",
    "        if  word_count > 110:\n",
    "            \n",
    "            toksList = sp.encode(a_sent,out_type=str) #\n",
    "\n",
    "            storeIndex = []\n",
    "            count = 0 #to store the index of '▁' occuring\n",
    "            \n",
    "            for i in toksList:\n",
    "                if i == '▁':\n",
    "                    storeIndex.append([i,count])\n",
    "                count = count + 1\n",
    "                \n",
    "            half = int(len(toksList)/2) #storing the index of mid length of all the '▁' indices\n",
    "\n",
    "            regex = re.compile('[,()}{:]')\n",
    "            \n",
    "            if(len(storeIndex) != 0):\n",
    "                for ind in storeIndex: #looping through all occurences of '▁' indices in the main toksList\n",
    "                    lstInd = ind[1] #index of the '▁' since ind is of the form: ['▁', 6]\n",
    "\n",
    "                    if(lstInd+1<len(toksList)):\n",
    "                        \n",
    "                        if((half <= lstInd) and (is_number(toksList[lstInd+1]) == False) and (regex.search(toksList[lstInd+1]) == None)): #first condition -> to make sure to split only after at least half the length. second condition -> to check if the next token is a number or not, if yes, do not split there. third condition -> to check if any brackets and anything else that is considered to be part of an entity is not split up.\n",
    "                            \n",
    "                            #performing detokenization\n",
    "                            x1 = \"\".join(toksList[:lstInd]) #adding all the tokens as a string till '▁' is encountered\n",
    "                            y1 = x1.replace('▁', ' ') #first part of the sentence\n",
    "\n",
    "                            tokenize_sentence.append(y1)\n",
    "                            \n",
    "                            #performing detokenization\n",
    "                            x2 = \"\".join(toksList[lstInd+1:]) #adding all the tokens as a string after '▁' is encountered\n",
    "                            y2 = x2.replace('▁', ' ') #second part of the sentence\n",
    "\n",
    "                            tokenize_sentence.append(y2)\n",
    "                            break #since we only want two parts\n",
    "      \n",
    "            \n",
    "        else:\n",
    "            tokenize_sentence.append(a_sent)\n",
    "       \n",
    "    return tokenize_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Adding Sentence Separator(@@@@@)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding sentence separator \n",
    "def addSplitter(raw_txt, final_txt_path):\n",
    "    final_txt = open(final_txt_path,'w')\n",
    "    with open(raw_txt,'r') as txt_file:\n",
    "        while True:\n",
    "            line = txt_file.readline()\n",
    "             \n",
    "            if not line: # if line is empty\n",
    "                break\n",
    "\n",
    "            if(line != \"\\n\"):\n",
    "                sent_split_list = sentence_tokenize(line)\n",
    "                for sent in sent_split_list:\n",
    "                    final_txt.write(sent)\n",
    "                    final_txt.write(\"@@@@@\")\n",
    "            else:\n",
    "                final_txt.write(line)\n",
    "    final_txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callForExtraction(json_path, converted_txt_path, final_txt_path): # extracting data from json to text file\n",
    "    extractJson(json_path,converted_txt_path)\n",
    "    addSplitter(converted_txt_path, final_txt_path) # after this, you will have a new file with sentence separators "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final code snippet to be called for export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Extracted\n"
     ]
    }
   ],
   "source": [
    "parent_path = Path(r'/Users/arya/Desktop/parentJSON') # JSON files' path\n",
    "target_path = Path(r'/Users/arya/Desktop/targetText') # brat input data(txt files) path\n",
    "for a_file in parent_path.rglob('*.json'):\n",
    "    converted_txt_path = Path(r'/Users/arya/Desktop/parentJSON/temp.txt') # change a path for temp file\n",
    "    json_path = a_file\n",
    "    a_file_name = a_file.name.replace(\".json\",\".txt\").replace(\" \",\"\")    \n",
    "    final_txt_path = os.path.join(target_path, a_file_name)\n",
    "    ann_file_name = a_file.name.replace(\".json\",\".ann\").replace(\" \",\"\")\n",
    "    ann_file_path =os.path.join(target_path, ann_file_name)\n",
    "    callForExtraction(json_path, converted_txt_path, final_txt_path)\n",
    "    open(ann_file_path, 'w').close()\n",
    "print(\"Data Extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
