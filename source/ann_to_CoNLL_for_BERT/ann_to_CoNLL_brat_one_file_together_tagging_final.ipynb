{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import json\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "import nltk\n",
    "import functools\n",
    "import tokenizations\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import os.path\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# starting CoNLL conversion from annotated file(.ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_ann(a_file): # reading the brat output annotation file\n",
    "    txt = open(a_file).read()       \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans(txt):\n",
    "    tokenizer = WordPunctTokenizer() # works best\n",
    "    tokens = tokenizer.tokenize(txt)\n",
    "    offset = 0\n",
    "    for token in tokens:\n",
    "        offset = txt.find(token, offset)\n",
    "        yield token, offset, offset+len(token)\n",
    "        offset += len(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_main_list(txt): # adding \"O\" tags to all tokens\n",
    "    main_list =[]\n",
    "    for token in spans(txt):\n",
    "        pos_tag = nltk.pos_tag([token[0]])[0]\n",
    "        main_list.append([token[1],token[0],pos_tag[1], \"O\"])\n",
    "    return main_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_file(filename): # checking validation\n",
    "    if(filename[len(filename)-4:len(filename)] != \".ann\"):\n",
    "        print(\"Invalid File\")\n",
    "        getInputFile()\n",
    "    else:\n",
    "        file = open(filename, \"r\")\n",
    "        return file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_separate_tags(tokens, main_list):\n",
    "    if 'R' in tokens[0]: # if it is a relationship link, ignore for now\n",
    "        return main_list\n",
    "    else:\n",
    "        if len(tokens) > 5:\n",
    "            temp = list()\n",
    "            temp = tokens[4:]\n",
    "            tag = tokens[1]\n",
    "            start = tokens[2]\n",
    "            end = tokens[3]\n",
    "            first_tok = tokens[4]\n",
    "            first_tag_string = \"B-\" + str(tag)\n",
    "            rest_tag_string = \"I-\" + str(tag)\n",
    "            flag = True\n",
    "            for ann_toks in temp:\n",
    "                # mapping words from entity list to main list and overlapping to create final list\n",
    "                for toks in main_list: \n",
    "                    if int(toks[0]) == int(start) and flag:\n",
    "                        toks[3] = first_tag_string\n",
    "                        flag = False\n",
    "                        break\n",
    "                # changing the main list token tags within the start and end range of the annotated tokens\n",
    "                    if toks[0] < int(end) and toks[0] >= int(start) and ann_toks == toks[1]:\n",
    "                        toks[3] = rest_tag_string\n",
    "                        break\n",
    "                        \n",
    "        else: # if there only is a single token annotated\n",
    "            tag = tokens[1]\n",
    "            start = tokens[2]\n",
    "            end = tokens[3]\n",
    "            first_tok = tokens[4]\n",
    "            first_tag_string = \"B-\" + str(tag)\n",
    "            for toks in main_list:\n",
    "                if int(toks[0]) == int(start):\n",
    "                    toks[3] = first_tag_string\n",
    "                    break\n",
    "    return main_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_lists(filename, main_list): # adding respective entity tags to annotated text\n",
    "    file = get_input_file(filename)\n",
    "    new_lst = []\n",
    "    for i in file:\n",
    "        tokenizer = WordPunctTokenizer() # using the same tokenizer as the main_list to avoid indexing issues\n",
    "        tokens = tokenizer.tokenize(i)\n",
    "        new_lst = generate_separate_tags(tokens, main_list)\n",
    "    return new_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_CoNLL(final_list): # converting to required CoNLL format\n",
    "    final_df = pd.DataFrame(final_list)\n",
    "    tokens = final_df[1]\n",
    "    pos_labels = final_df[2]\n",
    "    entity_labels = final_df[3]\n",
    "    conll_lines = \"\"\n",
    "    doc_start = \"-DOCSTART- -X- -X- O\\n\" \n",
    "    conll_lines += \"{}\\n\".format(doc_start)\n",
    "\n",
    "    for tokens, pos, labels in zip(tokens,pos_labels, entity_labels):\n",
    "        if \"@@@@@\" in tokens:\n",
    "            sent_start = \"\" \n",
    "            \n",
    "            conll_lines += \"{}\\n\".format(sent_start)\n",
    "        \n",
    "        else:\n",
    "            conll_lines += \"{} {} {} {}\\n\".format(tokens,pos,pos,labels)\n",
    "    return conll_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_txtfile(conll_lines, target_file): # helper to export the written CoNLL file\n",
    "    with open(target_file,\"w\") as txtfile:\n",
    "        for line in conll_lines:\n",
    "            txtfile.write(line)\n",
    "    txtfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callsForConversion(final_txt_path, input_ann, output_conll):\n",
    "    txt = read_input_ann(final_txt_path)\n",
    "    main_list = generate_main_list(txt)\n",
    "    final_list = map_lists(input_ann, main_list)\n",
    "    conll_lines = export_to_CoNLL(final_list)\n",
    "    export_txtfile(conll_lines, output_conll) # after this, you will have the fully ready CoNLL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Converted to CoNLL Successfully\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    final_txt_path = '/Users/arya/Desktop/AbhiramSinghvsC.D.Commachen&Orson16April,1996.txt'\n",
    "    output_conll = '/Users/arya/Desktop/AbhiramSinghCoNLL.txt'\n",
    "    input_ann = '/Users/arya/Desktop/AbhiramSinghvsC.D.Commachen&Orson16April,1996.ann'\n",
    "\n",
    "    callsForConversion(final_txt_path, input_ann, output_conll)\n",
    "    print(\"Data Converted to CoNLL Successfully\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
